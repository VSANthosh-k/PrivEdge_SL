{"cells":[{"cell_type":"markdown","metadata":{"id":"waECmNJKoSJ-"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DF-czaxWx8ii"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import pathlib\n","import socket, struct\n","import imblearn\n","from imblearn.under_sampling import RandomUnderSampler\n","from sklearn.preprocessing import LabelEncoder\n","from collections import Counter\n","from imblearn.over_sampling import SMOTE\n","from imblearn.pipeline import make_pipeline\n","from imblearn.under_sampling import NearMiss\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.feature_selection import RFE\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.preprocessing import LabelEncoder, StandardScaler, PowerTransformer\n","from sklearn.decomposition import PCA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iyvarGgDIG2L"},"outputs":[],"source":["import socket\n","import threading\n","import pickle\n","import numpy as np\n","from keras.models import Sequential\n","from keras.layers import Dense, Conv1D, Flatten, Input\n","from keras.optimizers import Adam\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ufn6sytvYTKA"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28511,"status":"ok","timestamp":1746681609987,"user":{"displayName":"V SANTHOSH KUMAR","userId":"08016301954865386445"},"user_tz":-330},"id":"QvSRXl8dy-st","outputId":"fac7bda2-6679-4137-f3d8-932fb60c7b9a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vLi0wjCSy-vr"},"outputs":[],"source":["benign = pathlib.Path(\"/content/drive/MyDrive/scopus/N-BaIoT dataset/1.benign.csv\")\n","g_c=pathlib.Path(\"/content/drive/MyDrive/scopus/N-BaIoT dataset/1.gafgyt.combo.csv\")\n","g_j=pathlib.Path(\"/content/drive/MyDrive/scopus/N-BaIoT dataset/1.gafgyt.junk.csv\")\n","g_s=pathlib.Path(\"/content/drive/MyDrive/scopus/N-BaIoT dataset/1.gafgyt.scan.csv\")\n","g_t=pathlib.Path(\"/content/drive/MyDrive/scopus/N-BaIoT dataset/1.gafgyt.tcp.csv\")\n","g_u=pathlib.Path(\"/content/drive/MyDrive/scopus/N-BaIoT dataset/1.gafgyt.udp.csv\")\n","m_a=pathlib.Path(\"/content/drive/MyDrive/scopus/N-BaIoT dataset/1.mirai.ack.csv\")\n","m_sc=pathlib.Path(\"/content/drive/MyDrive/scopus/N-BaIoT dataset/1.mirai.scan.csv\")\n","m_sy=pathlib.Path(\"/content/drive/MyDrive/scopus/N-BaIoT dataset/1.mirai.syn.csv\")\n","m_u=pathlib.Path(\"/content/drive/MyDrive/scopus/N-BaIoT dataset/1.mirai.udp.csv\")\n","m_u_p=pathlib.Path(\"/content/drive/MyDrive/scopus/N-BaIoT dataset/1.mirai.udpplain.csv\")\n","\n","benign9 = pathlib.Path(\"/content/drive/MyDrive/scopus/N-BaIoT dataset/9.benign.csv\")\n","g_c9=pathlib.Path(\"/content/drive/MyDrive/scopus/N-BaIoT dataset/9.gafgyt.combo.csv\")\n","g_j9=pathlib.Path(\"/content/drive/MyDrive/scopus/N-BaIoT dataset/9.gafgyt.junk.csv\")\n","g_s9=pathlib.Path(\"/content/drive/MyDrive/scopus/N-BaIoT dataset/9.gafgyt.scan.csv\")\n","g_t9=pathlib.Path(\"/content/drive/MyDrive/scopus/N-BaIoT dataset/9.gafgyt.tcp.csv\")\n","g_u9=pathlib.Path(\"/content/drive/MyDrive/scopus/N-BaIoT dataset/9.gafgyt.udp.csv\")\n","m_a9=pathlib.Path(\"/content/drive/MyDrive/scopus/N-BaIoT dataset/9.mirai.ack.csv\")\n","m_sc9=pathlib.Path(\"/content/drive/MyDrive/scopus/N-BaIoT dataset/9.mirai.scan.csv\")\n","m_sy9=pathlib.Path(\"/content/drive/MyDrive/scopus/N-BaIoT dataset/9.mirai.syn.csv\")\n","m_u9=pathlib.Path(\"/content/drive/MyDrive/scopus/N-BaIoT dataset/9.mirai.udp.csv\")\n","m_u_p9=pathlib.Path(\"/content/drive/MyDrive/scopus/N-BaIoT dataset/9.mirai.udpplain.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Onow6NoZETOu"},"outputs":[],"source":["# Load your data\n","benign = pd.read_csv(benign)\n","g_c=pd.read_csv(g_c)\n","g_j=pd.read_csv(g_j)\n","g_s=pd.read_csv(g_s)\n","g_t=pd.read_csv(g_t)\n","g_u=pd.read_csv(g_u)\n","m_a=pd.read_csv(m_a)\n","m_sc=pd.read_csv(m_sc)\n","m_sy=pd.read_csv(m_sy)\n","m_u=pd.read_csv(m_u)\n","m_u_p=pd.read_csv(m_u_p)\n","\n","benign9 = pd.read_csv(benign9)\n","g_c9=pd.read_csv(g_c9)\n","g_j9=pd.read_csv(g_j9)\n","g_s9=pd.read_csv(g_s9)\n","g_t9=pd.read_csv(g_t9)\n","g_u9=pd.read_csv(g_u9)\n","m_a9=pd.read_csv(m_a9)\n","m_sc9=pd.read_csv(m_sc9)\n","m_sy9=pd.read_csv(m_sy9)\n","m_u9=pd.read_csv(m_u9)\n","m_u_p9=pd.read_csv(m_u_p9)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q8gtsdZRy-yy"},"outputs":[],"source":["benign=benign.sample(frac=0.35,replace=False)\n","g_c=g_c.sample(frac=0.25,replace=False)\n","g_j=g_j.sample(frac=0.5,replace=False)\n","g_s=g_s.sample(frac=0.5,replace=False)\n","g_t=g_t.sample(frac=0.15,replace=False)\n","g_u=g_u.sample(frac=0.15,replace=False)\n","m_a=m_a.sample(frac=0.15,replace=False)\n","m_sc=m_sc.sample(frac=0.15,replace=False)\n","m_sy=m_sy.sample(frac=0.15,replace=False)\n","m_u=m_u.sample(frac=0.08,replace=False)\n","m_u_p=m_u_p.sample(frac=0.17,replace=False)\n","\n","benign9=benign9.sample(frac=0.35,replace=False)\n","g_c9=g_c9.sample(frac=0.25,replace=False)\n","g_j9=g_j9.sample(frac=0.5,replace=False)\n","g_s9=g_s9.sample(frac=0.5,replace=False)\n","g_t9=g_t9.sample(frac=0.15,replace=False)\n","g_u9=g_u9.sample(frac=0.15,replace=False)\n","m_a9=m_a9.sample(frac=0.15,replace=False)\n","m_sc9=m_sc9.sample(frac=0.15,replace=False)\n","m_sy9=m_sy9.sample(frac=0.15,replace=False)\n","m_u9=m_u9.sample(frac=0.08,replace=False)\n","m_u_p9=m_u_p9.sample(frac=0.17,replace=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0duD_tGsGoR7"},"outputs":[],"source":["benign['type']='benign'\n","m_u['type']='mirai_udp'\n","g_c['type']='gafgyt_combo'\n","g_j['type']='gafgyt_junk'\n","g_s['type']='gafgyt_scan'\n","g_t['type']='gafgyt_tcp'\n","g_u['type']='gafgyt_udp'\n","m_a['type']='mirai_ack'\n","m_sc['type']='mirai_scan'\n","m_sy['type']='mirai_syn'\n","m_u_p['type']='mirai_udpplain'\n","\n","benign9['type']='benign'\n","m_u9['type']='mirai_udp'\n","g_c9['type']='gafgyt_combo'\n","g_j9['type']='gafgyt_junk'\n","g_s9['type']='gafgyt_scan'\n","g_t9['type']='gafgyt_tcp'\n","g_u9['type']='gafgyt_udp'\n","m_a9['type']='mirai_ack'\n","m_sc9['type']='mirai_scan'\n","m_sy9['type']='mirai_syn'\n","m_u_p9['type']='mirai_udpplain'\n","\n","data=pd.concat([benign,m_u,g_c,g_j,g_s,g_t,g_u,m_a,m_sc,m_sy,m_u_p,benign9,m_u9,g_c9,g_j9,g_s9,g_t9,g_u9,m_a9,m_sc9,m_sy9,m_u_p9],\n","               axis=0, sort=False, ignore_index=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1746681700210,"user":{"displayName":"V SANTHOSH KUMAR","userId":"08016301954865386445"},"user_tz":-330},"id":"ftUFFIMQQ-5H","outputId":"d21013b7-694d-4a2b-9aa8-e3b740693319"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(322007, 116)"]},"metadata":{},"execution_count":9}],"source":["data.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38,"status":"ok","timestamp":1746681700940,"user":{"displayName":"V SANTHOSH KUMAR","userId":"08016301954865386445"},"user_tz":-330},"id":"pbuYEnWapEBj","outputId":"4e8c9237-f4fa-4eb3-a7da-ccadc2b38ee4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['benign', 'mirai_udp', 'gafgyt_combo', 'gafgyt_junk',\n","       'gafgyt_scan', 'gafgyt_tcp', 'gafgyt_udp', 'mirai_ack',\n","       'mirai_scan', 'mirai_syn', 'mirai_udpplain'], dtype=object)"]},"metadata":{},"execution_count":10}],"source":["data.type.unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":460},"executionInfo":{"elapsed":47,"status":"ok","timestamp":1746681702481,"user":{"displayName":"V SANTHOSH KUMAR","userId":"08016301954865386445"},"user_tz":-330},"id":"vROqjqRoGoU-","outputId":"38a53810-764a-42f1-85eb-54dcad5e47d3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["type\n","benign            24177\n","gafgyt_combo      29780\n","gafgyt_junk       28240\n","gafgyt_scan       29210\n","gafgyt_tcp        28532\n","gafgyt_udp        31328\n","mirai_ack         31407\n","mirai_scan        22704\n","mirai_syn         36758\n","mirai_udp         31580\n","mirai_udpplain    28291\n","Name: type, dtype: int64"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>type</th>\n","    </tr>\n","    <tr>\n","      <th>type</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>benign</th>\n","      <td>24177</td>\n","    </tr>\n","    <tr>\n","      <th>gafgyt_combo</th>\n","      <td>29780</td>\n","    </tr>\n","    <tr>\n","      <th>gafgyt_junk</th>\n","      <td>28240</td>\n","    </tr>\n","    <tr>\n","      <th>gafgyt_scan</th>\n","      <td>29210</td>\n","    </tr>\n","    <tr>\n","      <th>gafgyt_tcp</th>\n","      <td>28532</td>\n","    </tr>\n","    <tr>\n","      <th>gafgyt_udp</th>\n","      <td>31328</td>\n","    </tr>\n","    <tr>\n","      <th>mirai_ack</th>\n","      <td>31407</td>\n","    </tr>\n","    <tr>\n","      <th>mirai_scan</th>\n","      <td>22704</td>\n","    </tr>\n","    <tr>\n","      <th>mirai_syn</th>\n","      <td>36758</td>\n","    </tr>\n","    <tr>\n","      <th>mirai_udp</th>\n","      <td>31580</td>\n","    </tr>\n","    <tr>\n","      <th>mirai_udpplain</th>\n","      <td>28291</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> int64</label>"]},"metadata":{},"execution_count":11}],"source":["#how many instances of each class\n","data.groupby('type')['type'].count()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":256},"executionInfo":{"elapsed":354,"status":"ok","timestamp":1746681705069,"user":{"displayName":"V SANTHOSH KUMAR","userId":"08016301954865386445"},"user_tz":-330},"id":"mIlf1rtfGodT","outputId":"4ef282b6-786c-4eef-e4f8-458c4ea62d73"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["        MI_dir_L5_weight  MI_dir_L5_mean  MI_dir_L5_variance  \\\n","199888        138.715233       74.008293            0.198995   \n","249794          1.000000       60.000000            0.000000   \n","84912           1.000000       60.000000            0.000000   \n","280402        148.644446      370.567334        60678.555409   \n","135908         65.057568       60.000055            0.000824   \n","\n","        MI_dir_L3_weight  MI_dir_L3_mean  MI_dir_L3_variance  \\\n","199888        244.586134       74.017753            0.440336   \n","249794          1.000000       60.000000            0.000000   \n","84912           1.000000       60.000000            0.000000   \n","280402        231.696630      396.830840        56950.691840   \n","135908         84.094131       60.001195            0.017927   \n","\n","        MI_dir_L1_weight  MI_dir_L1_mean  MI_dir_L1_variance  \\\n","199888        786.952299       74.052156            5.962037   \n","249794          1.000000       60.000000            0.000000   \n","84912           1.000000       60.000000            0.000000   \n","280402        638.459353      429.780372        50331.722845   \n","135908        221.227846       60.012787            0.191658   \n","\n","        MI_dir_L0.1_weight  ...  HpHp_L0.1_covariance  HpHp_L0.1_pcc  \\\n","199888         4498.619555  ...                   0.0            0.0   \n","249794            1.000000  ...                   0.0            0.0   \n","84912             1.000000  ...                   0.0            0.0   \n","280402         6123.644401  ...                   0.0            0.0   \n","135908         2267.695057  ...                   0.0            0.0   \n","\n","        HpHp_L0.01_weight  HpHp_L0.01_mean  HpHp_L0.01_std  \\\n","199888                1.0             74.0             0.0   \n","249794                1.0             60.0             0.0   \n","84912                 1.0             60.0             0.0   \n","280402                1.0             60.0             0.0   \n","135908                1.0             60.0             0.0   \n","\n","        HpHp_L0.01_magnitude  HpHp_L0.01_radius  HpHp_L0.01_covariance  \\\n","199888                  74.0                0.0                    0.0   \n","249794                  60.0                0.0                    0.0   \n","84912                   60.0                0.0                    0.0   \n","280402                  60.0                0.0                    0.0   \n","135908                  60.0                0.0                    0.0   \n","\n","        HpHp_L0.01_pcc          type  \n","199888             0.0  gafgyt_combo  \n","249794             0.0    gafgyt_tcp  \n","84912              0.0    gafgyt_tcp  \n","280402             0.0     mirai_ack  \n","135908             0.0    mirai_scan  \n","\n","[5 rows x 116 columns]"],"text/html":["\n","  <div id=\"df-ead061d7-a953-4dc5-a10f-1986e9076ba0\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>MI_dir_L5_weight</th>\n","      <th>MI_dir_L5_mean</th>\n","      <th>MI_dir_L5_variance</th>\n","      <th>MI_dir_L3_weight</th>\n","      <th>MI_dir_L3_mean</th>\n","      <th>MI_dir_L3_variance</th>\n","      <th>MI_dir_L1_weight</th>\n","      <th>MI_dir_L1_mean</th>\n","      <th>MI_dir_L1_variance</th>\n","      <th>MI_dir_L0.1_weight</th>\n","      <th>...</th>\n","      <th>HpHp_L0.1_covariance</th>\n","      <th>HpHp_L0.1_pcc</th>\n","      <th>HpHp_L0.01_weight</th>\n","      <th>HpHp_L0.01_mean</th>\n","      <th>HpHp_L0.01_std</th>\n","      <th>HpHp_L0.01_magnitude</th>\n","      <th>HpHp_L0.01_radius</th>\n","      <th>HpHp_L0.01_covariance</th>\n","      <th>HpHp_L0.01_pcc</th>\n","      <th>type</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>199888</th>\n","      <td>138.715233</td>\n","      <td>74.008293</td>\n","      <td>0.198995</td>\n","      <td>244.586134</td>\n","      <td>74.017753</td>\n","      <td>0.440336</td>\n","      <td>786.952299</td>\n","      <td>74.052156</td>\n","      <td>5.962037</td>\n","      <td>4498.619555</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>74.0</td>\n","      <td>0.0</td>\n","      <td>74.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>gafgyt_combo</td>\n","    </tr>\n","    <tr>\n","      <th>249794</th>\n","      <td>1.000000</td>\n","      <td>60.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>60.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>60.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>60.0</td>\n","      <td>0.0</td>\n","      <td>60.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>gafgyt_tcp</td>\n","    </tr>\n","    <tr>\n","      <th>84912</th>\n","      <td>1.000000</td>\n","      <td>60.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>60.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>60.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>60.0</td>\n","      <td>0.0</td>\n","      <td>60.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>gafgyt_tcp</td>\n","    </tr>\n","    <tr>\n","      <th>280402</th>\n","      <td>148.644446</td>\n","      <td>370.567334</td>\n","      <td>60678.555409</td>\n","      <td>231.696630</td>\n","      <td>396.830840</td>\n","      <td>56950.691840</td>\n","      <td>638.459353</td>\n","      <td>429.780372</td>\n","      <td>50331.722845</td>\n","      <td>6123.644401</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>60.0</td>\n","      <td>0.0</td>\n","      <td>60.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>mirai_ack</td>\n","    </tr>\n","    <tr>\n","      <th>135908</th>\n","      <td>65.057568</td>\n","      <td>60.000055</td>\n","      <td>0.000824</td>\n","      <td>84.094131</td>\n","      <td>60.001195</td>\n","      <td>0.017927</td>\n","      <td>221.227846</td>\n","      <td>60.012787</td>\n","      <td>0.191658</td>\n","      <td>2267.695057</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>60.0</td>\n","      <td>0.0</td>\n","      <td>60.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>mirai_scan</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 116 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ead061d7-a953-4dc5-a10f-1986e9076ba0')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-ead061d7-a953-4dc5-a10f-1986e9076ba0 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-ead061d7-a953-4dc5-a10f-1986e9076ba0');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-9ee1d97b-9daa-4e46-a26b-33e9b965f827\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9ee1d97b-9daa-4e46-a26b-33e9b965f827')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-9ee1d97b-9daa-4e46-a26b-33e9b965f827 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"data"}},"metadata":{},"execution_count":12}],"source":["#shuffle rows of dataframe\n","sampler=np.random.permutation(len(data))\n","data=data.take(sampler)\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vxRacyPkwgfH"},"outputs":[],"source":["Y=data['type']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":256},"executionInfo":{"elapsed":146,"status":"ok","timestamp":1746681707264,"user":{"displayName":"V SANTHOSH KUMAR","userId":"08016301954865386445"},"user_tz":-330},"id":"UBhpnzcKImL4","outputId":"b9eeba32-c697-4a0a-c5ba-83ec2fc25c8f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["        MI_dir_L5_weight  MI_dir_L5_mean  MI_dir_L5_variance  \\\n","199888        138.715233       74.008293            0.198995   \n","249794          1.000000       60.000000            0.000000   \n","84912           1.000000       60.000000            0.000000   \n","280402        148.644446      370.567334        60678.555409   \n","135908         65.057568       60.000055            0.000824   \n","\n","        MI_dir_L3_weight  MI_dir_L3_mean  MI_dir_L3_variance  \\\n","199888        244.586134       74.017753            0.440336   \n","249794          1.000000       60.000000            0.000000   \n","84912           1.000000       60.000000            0.000000   \n","280402        231.696630      396.830840        56950.691840   \n","135908         84.094131       60.001195            0.017927   \n","\n","        MI_dir_L1_weight  MI_dir_L1_mean  MI_dir_L1_variance  \\\n","199888        786.952299       74.052156            5.962037   \n","249794          1.000000       60.000000            0.000000   \n","84912           1.000000       60.000000            0.000000   \n","280402        638.459353      429.780372        50331.722845   \n","135908        221.227846       60.012787            0.191658   \n","\n","        MI_dir_L0.1_weight  ...  HpHp_L0.1_radius  HpHp_L0.1_covariance  \\\n","199888         4498.619555  ...               0.0                   0.0   \n","249794            1.000000  ...               0.0                   0.0   \n","84912             1.000000  ...               0.0                   0.0   \n","280402         6123.644401  ...               0.0                   0.0   \n","135908         2267.695057  ...               0.0                   0.0   \n","\n","        HpHp_L0.1_pcc  HpHp_L0.01_weight  HpHp_L0.01_mean  HpHp_L0.01_std  \\\n","199888            0.0                1.0             74.0             0.0   \n","249794            0.0                1.0             60.0             0.0   \n","84912             0.0                1.0             60.0             0.0   \n","280402            0.0                1.0             60.0             0.0   \n","135908            0.0                1.0             60.0             0.0   \n","\n","        HpHp_L0.01_magnitude  HpHp_L0.01_radius  HpHp_L0.01_covariance  \\\n","199888                  74.0                0.0                    0.0   \n","249794                  60.0                0.0                    0.0   \n","84912                   60.0                0.0                    0.0   \n","280402                  60.0                0.0                    0.0   \n","135908                  60.0                0.0                    0.0   \n","\n","        HpHp_L0.01_pcc  \n","199888             0.0  \n","249794             0.0  \n","84912              0.0  \n","280402             0.0  \n","135908             0.0  \n","\n","[5 rows x 115 columns]"],"text/html":["\n","  <div id=\"df-e9d71c9b-b391-47f8-9f7d-1cb33e1e58f3\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>MI_dir_L5_weight</th>\n","      <th>MI_dir_L5_mean</th>\n","      <th>MI_dir_L5_variance</th>\n","      <th>MI_dir_L3_weight</th>\n","      <th>MI_dir_L3_mean</th>\n","      <th>MI_dir_L3_variance</th>\n","      <th>MI_dir_L1_weight</th>\n","      <th>MI_dir_L1_mean</th>\n","      <th>MI_dir_L1_variance</th>\n","      <th>MI_dir_L0.1_weight</th>\n","      <th>...</th>\n","      <th>HpHp_L0.1_radius</th>\n","      <th>HpHp_L0.1_covariance</th>\n","      <th>HpHp_L0.1_pcc</th>\n","      <th>HpHp_L0.01_weight</th>\n","      <th>HpHp_L0.01_mean</th>\n","      <th>HpHp_L0.01_std</th>\n","      <th>HpHp_L0.01_magnitude</th>\n","      <th>HpHp_L0.01_radius</th>\n","      <th>HpHp_L0.01_covariance</th>\n","      <th>HpHp_L0.01_pcc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>199888</th>\n","      <td>138.715233</td>\n","      <td>74.008293</td>\n","      <td>0.198995</td>\n","      <td>244.586134</td>\n","      <td>74.017753</td>\n","      <td>0.440336</td>\n","      <td>786.952299</td>\n","      <td>74.052156</td>\n","      <td>5.962037</td>\n","      <td>4498.619555</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>74.0</td>\n","      <td>0.0</td>\n","      <td>74.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>249794</th>\n","      <td>1.000000</td>\n","      <td>60.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>60.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>60.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>60.0</td>\n","      <td>0.0</td>\n","      <td>60.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>84912</th>\n","      <td>1.000000</td>\n","      <td>60.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>60.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>60.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>60.0</td>\n","      <td>0.0</td>\n","      <td>60.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>280402</th>\n","      <td>148.644446</td>\n","      <td>370.567334</td>\n","      <td>60678.555409</td>\n","      <td>231.696630</td>\n","      <td>396.830840</td>\n","      <td>56950.691840</td>\n","      <td>638.459353</td>\n","      <td>429.780372</td>\n","      <td>50331.722845</td>\n","      <td>6123.644401</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>60.0</td>\n","      <td>0.0</td>\n","      <td>60.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>135908</th>\n","      <td>65.057568</td>\n","      <td>60.000055</td>\n","      <td>0.000824</td>\n","      <td>84.094131</td>\n","      <td>60.001195</td>\n","      <td>0.017927</td>\n","      <td>221.227846</td>\n","      <td>60.012787</td>\n","      <td>0.191658</td>\n","      <td>2267.695057</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>60.0</td>\n","      <td>0.0</td>\n","      <td>60.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 115 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e9d71c9b-b391-47f8-9f7d-1cb33e1e58f3')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-e9d71c9b-b391-47f8-9f7d-1cb33e1e58f3 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-e9d71c9b-b391-47f8-9f7d-1cb33e1e58f3');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-01091222-7d36-4d6d-8a90-bdfafebc852e\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-01091222-7d36-4d6d-8a90-bdfafebc852e')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-01091222-7d36-4d6d-8a90-bdfafebc852e button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"X"}},"metadata":{},"execution_count":14}],"source":["#drop labels from training dataset\n","X=data.drop(columns='type')\n","X.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1746681708978,"user":{"displayName":"V SANTHOSH KUMAR","userId":"08016301954865386445"},"user_tz":-330},"id":"rlkwKK7Ep6vF","outputId":"d51e706b-123c-4935-f198-80d50f3842d8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(322007, 115)"]},"metadata":{},"execution_count":15}],"source":["X.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1746681709573,"user":{"displayName":"V SANTHOSH KUMAR","userId":"08016301954865386445"},"user_tz":-330},"id":"CC1qGJ54xDkQ","outputId":"95abea56-99c2-4ed2-cd4b-0b6de6f3d549"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(322007,)"]},"metadata":{},"execution_count":16}],"source":["Y.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":241},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1746681711097,"user":{"displayName":"V SANTHOSH KUMAR","userId":"08016301954865386445"},"user_tz":-330},"id":"tUkMfaaVdC9U","outputId":"f63d109d-ac21-488b-a261-4714a79b7d78"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["199888    gafgyt_combo\n","249794      gafgyt_tcp\n","84912       gafgyt_tcp\n","280402       mirai_ack\n","135908      mirai_scan\n","Name: type, dtype: object"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>type</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>199888</th>\n","      <td>gafgyt_combo</td>\n","    </tr>\n","    <tr>\n","      <th>249794</th>\n","      <td>gafgyt_tcp</td>\n","    </tr>\n","    <tr>\n","      <th>84912</th>\n","      <td>gafgyt_tcp</td>\n","    </tr>\n","    <tr>\n","      <th>280402</th>\n","      <td>mirai_ack</td>\n","    </tr>\n","    <tr>\n","      <th>135908</th>\n","      <td>mirai_scan</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> object</label>"]},"metadata":{},"execution_count":17}],"source":["Y.head()"]},{"cell_type":"markdown","metadata":{"id":"zMLhU6IhVNF3"},"source":["**ANOVA Feature selection method**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qk0Tw7enPECp"},"outputs":[],"source":["from sklearn.feature_selection import SelectKBest, SelectPercentile, chi2, f_classif, mutual_info_classif\n","from sklearn.feature_selection import VarianceThreshold\n","from sklearn.preprocessing import StandardScaler, LabelEncoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mQCn63skWI6r"},"outputs":[],"source":["# Apply SelectKBest with the ANOVA F-test\n","k = 40  # Define the number of top features to select\n","selector = SelectKBest(score_func=f_classif, k=k)\n","X_selected_A = selector.fit_transform(X,Y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5eCGcXp8VLNS"},"outputs":[],"source":["# Get selected feature indices and scores\n","selected_indices = selector.get_support(indices=True)\n","selected_feature_names = X.columns[selected_indices]\n","selected_scores = selector.scores_[selected_indices]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34,"status":"ok","timestamp":1746681714971,"user":{"displayName":"V SANTHOSH KUMAR","userId":"08016301954865386445"},"user_tz":-330},"id":"csi2pbRsVLRD","outputId":"30ed87bf-6024-46b4-a041-ea83447ff457"},"outputs":[{"output_type":"stream","name":"stdout","text":["Selected top-k features based on ANOVA F-test:\n","Feature: MI_dir_L5_weight, Score: 167468.76584447137\n","Feature: MI_dir_L5_mean, Score: 185937.49546722524\n","Feature: MI_dir_L5_variance, Score: 130445.72101588172\n","Feature: MI_dir_L3_weight, Score: 282759.3119613909\n","Feature: MI_dir_L3_mean, Score: 309356.0808118814\n","Feature: MI_dir_L3_variance, Score: 283844.3851075396\n","Feature: MI_dir_L1_weight, Score: 589245.7081541196\n","Feature: MI_dir_L1_mean, Score: 652322.2417386321\n","Feature: MI_dir_L1_variance, Score: 1023740.2184169543\n","Feature: MI_dir_L0.1_weight, Score: 476786.14680377644\n","Feature: MI_dir_L0.1_mean, Score: 1130158.2613736007\n","Feature: MI_dir_L0.1_variance, Score: 1858046.4500449367\n","Feature: MI_dir_L0.01_weight, Score: 92482.42759432855\n","Feature: MI_dir_L0.01_mean, Score: 1485578.4738247753\n","Feature: MI_dir_L0.01_variance, Score: 1796529.2980952852\n","Feature: H_L5_weight, Score: 167468.76584447065\n","Feature: H_L5_mean, Score: 185937.4954672774\n","Feature: H_L5_variance, Score: 130445.72101588166\n","Feature: H_L3_weight, Score: 282759.31196002563\n","Feature: H_L3_mean, Score: 309356.08094538323\n","Feature: H_L3_variance, Score: 283844.3851071043\n","Feature: H_L1_weight, Score: 589245.7072005539\n","Feature: H_L1_mean, Score: 652322.8249140658\n","Feature: H_L1_variance, Score: 1023740.2187354541\n","Feature: H_L0.1_weight, Score: 476786.14144208346\n","Feature: H_L0.1_mean, Score: 1130538.9487388784\n","Feature: H_L0.1_variance, Score: 1858053.4968749296\n","Feature: H_L0.01_weight, Score: 92482.42680044504\n","Feature: H_L0.01_mean, Score: 1488263.317163121\n","Feature: H_L0.01_variance, Score: 1796491.9339231057\n","Feature: HH_L5_weight, Score: 108229.86751054438\n","Feature: HH_L3_weight, Score: 116207.93039314185\n","Feature: HH_L1_weight, Score: 121897.36354836152\n","Feature: HH_L0.1_weight, Score: 86242.76120823398\n","Feature: HH_jit_L5_weight, Score: 108229.86751054453\n","Feature: HH_jit_L3_weight, Score: 116207.93039314194\n","Feature: HH_jit_L1_weight, Score: 121897.36354836175\n","Feature: HH_jit_L0.1_weight, Score: 86242.76120823492\n","Feature: HH_jit_L0.01_mean, Score: 47445.31868492318\n","Feature: HH_jit_L0.01_variance, Score: 46534.436988908055\n"]}],"source":["# Display selected features and their ANOVA F-scores\n","print(\"Selected top-k features based on ANOVA F-test:\")\n","for feature, score in zip(selected_feature_names, selected_scores):\n","    print(f\"Feature: {feature}, Score: {score}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NEQki2ybVLUc"},"outputs":[],"source":["# Create a DataFrame with selected features\n","X_selected_dfA = pd.DataFrame(X_selected_A, columns=selected_feature_names)\n","\n","selected_data_A = pd.concat([X_selected_dfA, Y.reset_index(drop=True)], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":290},"executionInfo":{"elapsed":53,"status":"ok","timestamp":1746681719655,"user":{"displayName":"V SANTHOSH KUMAR","userId":"08016301954865386445"},"user_tz":-330},"id":"7ePcJnDMVLX1","outputId":"98dd37eb-f652-462e-eef6-101f17afeed8"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","DataFrame with selected features and target label:\n"]},{"output_type":"execute_result","data":{"text/plain":["   MI_dir_L5_weight  MI_dir_L5_mean  MI_dir_L5_variance  MI_dir_L3_weight  \\\n","0        138.715233       74.008293            0.198995        244.586134   \n","1          1.000000       60.000000            0.000000          1.000000   \n","2          1.000000       60.000000            0.000000          1.000000   \n","3        148.644446      370.567334        60678.555409        231.696630   \n","4         65.057568       60.000055            0.000824         84.094131   \n","\n","   MI_dir_L3_mean  MI_dir_L3_variance  MI_dir_L1_weight  MI_dir_L1_mean  \\\n","0       74.017753            0.440336        786.952299       74.052156   \n","1       60.000000            0.000000          1.000000       60.000000   \n","2       60.000000            0.000000          1.000000       60.000000   \n","3      396.830840        56950.691840        638.459353      429.780372   \n","4       60.001195            0.017927        221.227846       60.012787   \n","\n","   MI_dir_L1_variance  MI_dir_L0.1_weight  ...  HH_L3_weight  HH_L1_weight  \\\n","0            5.962037         4498.619555  ...    244.405787    785.806041   \n","1            0.000000            1.000000  ...      1.000000      1.000000   \n","2            0.000000            1.000000  ...      1.000000      1.000000   \n","3        50331.722845         6123.644401  ...      1.000000      1.000000   \n","4            0.191658         2267.695057  ...      1.000000      1.000000   \n","\n","   HH_L0.1_weight  HH_jit_L5_weight  HH_jit_L3_weight  HH_jit_L1_weight  \\\n","0     4487.247188          138.6673        244.405787        785.806041   \n","1        1.000000            1.0000          1.000000          1.000000   \n","2        1.000000            1.0000          1.000000          1.000000   \n","3        1.000000            1.0000          1.000000          1.000000   \n","4        1.000000            1.0000          1.000000          1.000000   \n","\n","   HH_jit_L0.1_weight  HH_jit_L0.01_mean  HH_jit_L0.01_variance          type  \n","0         4487.247188       2.192049e+05           3.300558e+14  gafgyt_combo  \n","1            1.000000       1.505914e+09           0.000000e+00    gafgyt_tcp  \n","2            1.000000       1.505914e+09           0.000000e+00    gafgyt_tcp  \n","3            1.000000       1.507659e+09           0.000000e+00     mirai_ack  \n","4            1.000000       1.507653e+09           0.000000e+00    mirai_scan  \n","\n","[5 rows x 41 columns]"],"text/html":["\n","  <div id=\"df-9dd0e8f4-d3f6-4c45-bb4b-2a91d569c089\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>MI_dir_L5_weight</th>\n","      <th>MI_dir_L5_mean</th>\n","      <th>MI_dir_L5_variance</th>\n","      <th>MI_dir_L3_weight</th>\n","      <th>MI_dir_L3_mean</th>\n","      <th>MI_dir_L3_variance</th>\n","      <th>MI_dir_L1_weight</th>\n","      <th>MI_dir_L1_mean</th>\n","      <th>MI_dir_L1_variance</th>\n","      <th>MI_dir_L0.1_weight</th>\n","      <th>...</th>\n","      <th>HH_L3_weight</th>\n","      <th>HH_L1_weight</th>\n","      <th>HH_L0.1_weight</th>\n","      <th>HH_jit_L5_weight</th>\n","      <th>HH_jit_L3_weight</th>\n","      <th>HH_jit_L1_weight</th>\n","      <th>HH_jit_L0.1_weight</th>\n","      <th>HH_jit_L0.01_mean</th>\n","      <th>HH_jit_L0.01_variance</th>\n","      <th>type</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>138.715233</td>\n","      <td>74.008293</td>\n","      <td>0.198995</td>\n","      <td>244.586134</td>\n","      <td>74.017753</td>\n","      <td>0.440336</td>\n","      <td>786.952299</td>\n","      <td>74.052156</td>\n","      <td>5.962037</td>\n","      <td>4498.619555</td>\n","      <td>...</td>\n","      <td>244.405787</td>\n","      <td>785.806041</td>\n","      <td>4487.247188</td>\n","      <td>138.6673</td>\n","      <td>244.405787</td>\n","      <td>785.806041</td>\n","      <td>4487.247188</td>\n","      <td>2.192049e+05</td>\n","      <td>3.300558e+14</td>\n","      <td>gafgyt_combo</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.000000</td>\n","      <td>60.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>60.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>60.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>...</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.0000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.505914e+09</td>\n","      <td>0.000000e+00</td>\n","      <td>gafgyt_tcp</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.000000</td>\n","      <td>60.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>60.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>60.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>...</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.0000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.505914e+09</td>\n","      <td>0.000000e+00</td>\n","      <td>gafgyt_tcp</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>148.644446</td>\n","      <td>370.567334</td>\n","      <td>60678.555409</td>\n","      <td>231.696630</td>\n","      <td>396.830840</td>\n","      <td>56950.691840</td>\n","      <td>638.459353</td>\n","      <td>429.780372</td>\n","      <td>50331.722845</td>\n","      <td>6123.644401</td>\n","      <td>...</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.0000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.507659e+09</td>\n","      <td>0.000000e+00</td>\n","      <td>mirai_ack</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>65.057568</td>\n","      <td>60.000055</td>\n","      <td>0.000824</td>\n","      <td>84.094131</td>\n","      <td>60.001195</td>\n","      <td>0.017927</td>\n","      <td>221.227846</td>\n","      <td>60.012787</td>\n","      <td>0.191658</td>\n","      <td>2267.695057</td>\n","      <td>...</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.0000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.507653e+09</td>\n","      <td>0.000000e+00</td>\n","      <td>mirai_scan</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 41 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9dd0e8f4-d3f6-4c45-bb4b-2a91d569c089')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-9dd0e8f4-d3f6-4c45-bb4b-2a91d569c089 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-9dd0e8f4-d3f6-4c45-bb4b-2a91d569c089');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-4609d515-0832-49f1-9486-28dd8eace567\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4609d515-0832-49f1-9486-28dd8eace567')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-4609d515-0832-49f1-9486-28dd8eace567 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"selected_data_A"}},"metadata":{},"execution_count":23}],"source":["print(\"\\nDataFrame with selected features and target label:\")\n","selected_data_A.head()"]},{"cell_type":"markdown","metadata":{"id":"VPJEX-D1nILU"},"source":["**Split learning Model using pearson ANOVA-F based feature selection**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D4234ejIzk5V"},"outputs":[],"source":["# Convert application names to numbers\n","encoder = LabelEncoder()\n","encoder.fit(Y)\n","encoded_Y = encoder.transform(Y)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":69,"status":"ok","timestamp":1746681722509,"user":{"displayName":"V SANTHOSH KUMAR","userId":"08016301954865386445"},"user_tz":-330},"id":"ODxE56TI0iWa","outputId":"c8cce7c0-03a8-48ee-8553-c872c8ccba91"},"outputs":[{"output_type":"stream","name":"stdout","text":["[('benign', 24177), ('gafgyt_combo', 29780), ('gafgyt_junk', 28240), ('gafgyt_scan', 29210), ('gafgyt_tcp', 28532), ('gafgyt_udp', 31328), ('mirai_ack', 31407), ('mirai_scan', 22704), ('mirai_syn', 36758), ('mirai_udp', 31580), ('mirai_udpplain', 28291)]\n"]}],"source":["print(sorted(Counter(Y).items()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hjLsV6E4zmLx"},"outputs":[],"source":["smote = SMOTE(sampling_strategy='auto', random_state=42)\n","X_resampled, y_resampled = smote.fit_resample(X_selected_dfA, encoded_Y)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1746681736761,"user":{"displayName":"V SANTHOSH KUMAR","userId":"08016301954865386445"},"user_tz":-330},"id":"RrychZk2zmO5","outputId":"50edb24e-b349-47c1-967f-c7274dde2bae"},"outputs":[{"output_type":"stream","name":"stdout","text":["[(np.int64(0), 24177), (np.int64(1), 29780), (np.int64(2), 28240), (np.int64(3), 29210), (np.int64(4), 28532), (np.int64(5), 31328), (np.int64(6), 31407), (np.int64(7), 22704), (np.int64(8), 36758), (np.int64(9), 31580), (np.int64(10), 28291)]\n"]}],"source":["print(sorted(Counter(encoded_Y).items()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1746681739117,"user":{"displayName":"V SANTHOSH KUMAR","userId":"08016301954865386445"},"user_tz":-330},"id":"gv3MHntt4qQz","outputId":"41eac8ba-2f21-4faa-f8e3-40215cf480bb"},"outputs":[{"output_type":"stream","name":"stdout","text":["[(np.int64(0), 36758), (np.int64(1), 36758), (np.int64(2), 36758), (np.int64(3), 36758), (np.int64(4), 36758), (np.int64(5), 36758), (np.int64(6), 36758), (np.int64(7), 36758), (np.int64(8), 36758), (np.int64(9), 36758), (np.int64(10), 36758)]\n"]}],"source":["print(sorted(Counter(y_resampled).items()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1746681739717,"user":{"displayName":"V SANTHOSH KUMAR","userId":"08016301954865386445"},"user_tz":-330},"id":"Huxo1pYzzmR1","outputId":"724b811d-70a3-468e-b71b-e02e091cf5ef"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 4,  4,  6,  7,  5,  6, 10,  8, 10])"]},"metadata":{},"execution_count":29}],"source":["encoded_Y[1:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41,"status":"ok","timestamp":1746681740819,"user":{"displayName":"V SANTHOSH KUMAR","userId":"08016301954865386445"},"user_tz":-330},"id":"MfV6rXCI1f_F","outputId":"a1726d3f-297a-4fab-cec4-0050de91657c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(404338, 40)"]},"metadata":{},"execution_count":30}],"source":["X_resampled.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1746681742407,"user":{"displayName":"V SANTHOSH KUMAR","userId":"08016301954865386445"},"user_tz":-330},"id":"Md3RPXLF1gCl","outputId":"d51d8bf4-39bd-48c5-814d-3a1e81de31bc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(404338,)"]},"metadata":{},"execution_count":31}],"source":[" y_resampled.shape\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Ihfq5rEVLg7"},"outputs":[],"source":["#split data\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test,= train_test_split(X_resampled, y_resampled, train_size = 0.70, random_state=100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mTG_CfygstP1"},"outputs":[],"source":["# Data Transformation to Reduce Skewness\n","power_transformer = PowerTransformer(method='yeo-johnson')\n","X_train_transformed = power_transformer.fit_transform(X_train)\n","X_test_transformed = power_transformer.transform(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X0aOdT1TstU4"},"outputs":[],"source":["# Standard Scaling After Transformation\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train_transformed)\n","X_test_scaled = scaler.transform(X_test_transformed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ZgGNdmcbzaO"},"outputs":[],"source":["y_train=tf.keras.utils.to_categorical(y_train, num_classes=11)\n","y_test = tf.keras.utils.to_categorical(y_test, num_classes=11)  # One-hot encoding for test labels"]},{"cell_type":"code","source":["y_train[1:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XRYljQRZ03TG","executionInfo":{"status":"ok","timestamp":1746681753445,"user_tz":-330,"elapsed":42,"user":{"displayName":"V SANTHOSH KUMAR","userId":"08016301954865386445"}},"outputId":"86c306f4-4823-41db-9342-61c3c5531ba5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n","       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n","       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n","       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6373,"status":"ok","timestamp":1746681761961,"user":{"displayName":"V SANTHOSH KUMAR","userId":"08016301954865386445"},"user_tz":-330},"id":"5KQZ8U8I7cCn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"30f2eee9-2df4-4c4c-ace0-882af6624ec4"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m116.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["! pip install -q tensorflow-model-optimization"]},{"cell_type":"markdown","source":["***Dt:23/04/25***\n"],"metadata":{"id":"obnaaCBj2QMx"}},{"cell_type":"code","source":[],"metadata":{"id":"E4ig60K1HPjd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UrmGwz6fJ4sa"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gG2zHtgtdW_I"},"outputs":[],"source":["import socket\n","import pickle\n","import zlib\n","import tensorflow as tf\n","import numpy as np\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import MaxPooling1D, Flatten, Dense, BatchNormalization\n","from tensorflow.keras.optimizers import Adam\n","import threading\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","from tensorflow.keras.layers import Dropout\n","\n","# Server-side model\n","def create_server_model():\n","    model = Sequential([\n","        MaxPooling1D(pool_size=2,input_shape=(20, 64)),\n","        Conv1D(128, kernel_size=3, activation='relu',padding='same'),\n","        BatchNormalization(),\n","        Conv1D(128, kernel_size=3, activation='relu',padding='same'),\n","        BatchNormalization(),\n","        MaxPooling1D(pool_size=2),\n","        Conv1D(256, kernel_size=3, activation='relu',padding='same'),\n","        BatchNormalization(),\n","        Conv1D(256, kernel_size=3, activation='relu',padding='same'),\n","        BatchNormalization(),\n","        MaxPooling1D(pool_size=2),\n","        Flatten(),\n","        Dense(256, activation='relu'),\n","        Dropout(0.3),\n","        Dense(128, activation='relu'),\n","        Dropout(0.2),\n","        Dense(11, activation='softmax')\n","    ])\n","    model.compile(optimizer=Adam(learning_rate=0.001),\n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","    return model\n","\n","# Deserialize and decompress data\n","def deserialize_data(data):\n","    return pickle.loads(zlib.decompress(data))\n","\n","# Server function\n","def start_server(host='127.0.0.1', port=65438):\n","    server_model = create_server_model()\n","    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n","    server_socket.bind((host, port))\n","    server_socket.listen(1)\n","    print(\"Server waiting for connection...\")\n","    conn, addr = server_socket.accept()\n","    print(f\"Connected to client: {addr}\")\n","    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n","    initial_learning_rate=0.001,\n","    decay_steps=10000,\n","    decay_rate=0.9\n","    )\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n","\n","    all_true_labels = []\n","    all_pred_labels = []\n","\n","\n","    while True:\n","        try:\n","            # Receive header indicating batch size\n","            header = conn.recv(8)\n","            if not header:\n","                break\n","            data_size = int(header.decode('utf-8').strip())\n","\n","\n","            # If a special flag (e.g., -1) is sent, switch to evaluation\n","            if data_size == -1:\n","              print(\"Switching to evaluation phase...\")\n","              break\n","\n","            # Receive the batch data\n","            data = b\"\"\n","            while len(data) < data_size:\n","                packet = conn.recv(4096)\n","                if not packet:\n","                    break\n","                data += packet\n","\n","            # Deserialize the data\n","            intermediate_output, labels = deserialize_data(data)\n","\n","            # Perform forward pass and backpropagation on batch\n","            with tf.GradientTape() as tape:\n","                logits = server_model(np.array(intermediate_output), training=True)\n","                loss = tf.reduce_mean(\n","                    tf.keras.losses.categorical_crossentropy(np.array(labels), logits)\n","                )\n","            gradients = tape.gradient(loss, server_model.trainable_variables)\n","            optimizer.apply_gradients(zip(gradients, server_model.trainable_variables))\n","\n","            pred_classes = np.argmax(logits.numpy(), axis=1)\n","            true_classes = np.argmax(labels, axis=1)\n","\n","            all_pred_labels.extend(pred_classes)\n","            all_true_labels.extend(true_classes)\n","\n","            correct_predictions = int(np.sum(\n","                np.argmax(logits.numpy(), axis=1) == np.argmax(labels, axis=1)\n","            ))\n","            response = f\"{loss.numpy():.4f},{correct_predictions}\"\n","            conn.sendall(f\"{len(response):08}\".encode('utf-8'))\n","            conn.sendall(response.encode('utf-8'))\n","\n","        except Exception as e:\n","            print(f\"Error: {e}\")\n","            break\n","\n","    # At the end of training\n","    precision = precision_score(all_true_labels, all_pred_labels, average='macro')\n","    recall = recall_score(all_true_labels, all_pred_labels, average='macro')\n","    f1 = f1_score(all_true_labels, all_pred_labels, average='macro')\n","    print(f\"\\nTraining Metrics → Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n","\n","    # Handle test evaluation\n","    print(\"Evaluating test data...\")\n","\n","    all_test_true = []\n","    all_test_pred = []\n","\n","    while True:\n","        header = conn.recv(8)\n","        if not header:\n","            break\n","        test_data_size = int(header.decode('utf-8').strip())\n","        test_data = b\"\"\n","        while len(test_data) < test_data_size:\n","            packet = conn.recv(4096)\n","            if not packet:\n","                break\n","            test_data += packet\n","\n","        test_intermediate, test_labels = deserialize_data(test_data)\n","        logits = server_model(np.array(test_intermediate), training=False)\n","\n","        test_intermediate = np.array(test_intermediate)\n","        test_labels = np.array(test_labels)\n","\n","        logits = server_model(test_intermediate, training=False)\n","        pred_classes = np.argmax(logits.numpy(), axis=1)\n","        true_classes = np.argmax(test_labels, axis=1)\n","\n","        all_test_pred.extend(pred_classes)\n","        all_test_true.extend(true_classes)\n","\n","        correct_predictions = int(np.sum(\n","            np.argmax(logits.numpy(), axis=1) == np.argmax(test_labels, axis=1)\n","        ))\n","        response = f\"{correct_predictions}\"\n","        conn.sendall(f\"{len(response):08}\".encode('utf-8'))\n","        conn.sendall(response.encode('utf-8'))\n","\n","    # Final test metrics\n","    precision = precision_score(all_test_true, all_test_pred, average='macro')\n","    recall = recall_score(all_test_true, all_test_pred, average='macro')\n","    f1 = f1_score(all_test_true, all_test_pred, average='macro')\n","    print(f\"\\nTesting Metrics → Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n","\n","\n","    conn.close()\n","    server_socket.close()\n","    print(\"Server connection closed.\")\n","\n","server_thread = threading.Thread(target=start_server)\n","server_thread.start()\n"]},{"cell_type":"code","source":[],"metadata":{"id":"GAjSknDvJ4xo"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0d40ae6b-2419-4a02-c127-c8ef569642f1","id":"3UjmKGLpda-M","executionInfo":{"status":"ok","timestamp":1745603612169,"user_tz":-330,"elapsed":6481956,"user":{"displayName":"V SANTHOSH KUMAR","userId":"08016301954865386445"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Connected to server!Connected to client: ('127.0.0.1', 53536)\n","\n","\n","Starting Training...\n","\n","Epoch 1/15\n","Batch 553/553 - Progress: 100.00% - Loss: 0.1736\n","Epoch 1 completed: Avg Loss = 0.2384, Training Accuracy = 0.8651\n","\n","Epoch 2/15\n","Batch 553/553 - Progress: 100.00% - Loss: 0.2064\n","Epoch 2 completed: Avg Loss = 0.1568, Training Accuracy = 0.8990\n","\n","Epoch 3/15\n","Batch 553/553 - Progress: 100.00% - Loss: 0.1391\n","Epoch 3 completed: Avg Loss = 0.1462, Training Accuracy = 0.9024\n","\n","Epoch 4/15\n","Batch 553/553 - Progress: 100.00% - Loss: 0.1337\n","Epoch 4 completed: Avg Loss = 0.1434, Training Accuracy = 0.9032\n","\n","Epoch 5/15\n","Batch 553/553 - Progress: 100.00% - Loss: 0.1416\n","Epoch 5 completed: Avg Loss = 0.1420, Training Accuracy = 0.9040\n","\n","Epoch 6/15\n","Batch 553/553 - Progress: 100.00% - Loss: 0.1266\n","Epoch 6 completed: Avg Loss = 0.1377, Training Accuracy = 0.9050\n","\n","Epoch 7/15\n","Batch 553/553 - Progress: 100.00% - Loss: 0.1213\n","Epoch 7 completed: Avg Loss = 0.1366, Training Accuracy = 0.9056\n","\n","Epoch 8/15\n","Batch 553/553 - Progress: 100.00% - Loss: 0.1206\n","Epoch 8 completed: Avg Loss = 0.1350, Training Accuracy = 0.9066\n","\n","Epoch 9/15\n","Batch 553/553 - Progress: 100.00% - Loss: 0.1255\n","Epoch 9 completed: Avg Loss = 0.1353, Training Accuracy = 0.9066\n","\n","Epoch 10/15\n","Batch 553/553 - Progress: 100.00% - Loss: 0.1251\n","Epoch 10 completed: Avg Loss = 0.1350, Training Accuracy = 0.9063\n","\n","Epoch 11/15\n","Batch 553/553 - Progress: 100.00% - Loss: 0.1289\n","Epoch 11 completed: Avg Loss = 0.1339, Training Accuracy = 0.9064\n","\n","Epoch 12/15\n","Batch 553/553 - Progress: 100.00% - Loss: 0.1208\n","Epoch 12 completed: Avg Loss = 0.1346, Training Accuracy = 0.9066\n","\n","Epoch 13/15\n","Batch 553/553 - Progress: 100.00% - Loss: 0.1227\n","Epoch 13 completed: Avg Loss = 0.1337, Training Accuracy = 0.9071\n","\n","Epoch 14/15\n","Batch 553/553 - Progress: 100.00% - Loss: 0.1223\n","Epoch 14 completed: Avg Loss = 0.1311, Training Accuracy = 0.9082\n","\n","Epoch 15/15\n","Batch 553/553 - Progress: 100.00% - Loss: 0.1231\n","Epoch 15 completed: Avg Loss = 0.1344, Training Accuracy = 0.9072\n","\n","Training completed in 8136.38 seconds.\n","Switching to evaluation phase...\n","Training complete. Signaled server to start evaluation.\n","\n","Evaluating on test data...\n","\n","Training Metrics → Precision: 0.9027, Recall: 0.9027, F1 Score: 0.9026\n","Evaluating test data...\n","\n","Testing completed: Testing Accuracy = 0.8923\n","\n","Client connection closed.\n"]}],"source":["import socket\n","import pickle\n","import zlib\n","import time\n","import tensorflow as tf\n","import numpy as np\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical\n","\n","# Client-side model\n","def create_client_model():\n","    model = Sequential([\n","        Conv1D(16, kernel_size=3, activation='relu', input_shape=(40, 1),padding='same'),\n","        BatchNormalization(),\n","        Conv1D(32, kernel_size=3, activation='relu',padding='same'),\n","        BatchNormalization(),\n","        #MaxPooling1D(pool_size=2),\n","        Conv1D(32, kernel_size=3, activation='relu',padding='same'),\n","        BatchNormalization(),\n","        #MaxPooling1D(pool_size=2),\n","        Conv1D(64, kernel_size=3, activation='relu',padding='same'),\n","        BatchNormalization(),\n","        Conv1D(64, kernel_size=3, activation='relu',padding='same'),\n","        BatchNormalization(),\n","        MaxPooling1D(pool_size=2),\n","\n","    ])\n","    return model\n","\n","# Serialize and compress data using pickle\n","def serialize_data(data):\n","    return zlib.compress(pickle.dumps(data))\n","\n","# Client function\n","def start_client(host='127.0.0.1', port=65438):\n","    client_model = create_client_model()\n","    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n","    client_socket.connect((host, port))\n","    print(\"Connected to server!\")\n","\n","    batch_size = 512\n","    epochs =  15\n","    total_batches = int(np.ceil(X_train_scaled.shape[0] / batch_size))\n","\n","    print(\"\\nStarting Training...\")\n","    start_time = time.time()\n","\n","    for epoch in range(epochs):\n","        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n","        epoch_loss = 0\n","        correct_predictions = 0\n","        total_samples = 0\n","\n","        for batch_index, i in enumerate(range(0, X_train_scaled.shape[0], batch_size), start=1):\n","            x_batch = X_train_scaled[i:min(i + batch_size, X_train_scaled.shape[0])]\n","            y_batch = y_train[i:min(i + batch_size, y_train.shape[0])]\n","\n","            # Forward pass to get intermediate activations\n","            intermediate_output = client_model(x_batch, training=True).numpy()\n","\n","            # Serialize and send data\n","            data = serialize_data((intermediate_output.tolist(), y_batch.tolist()))\n","            client_socket.sendall(f\"{len(data):08}\".encode('utf-8'))\n","            client_socket.sendall(data)\n","\n","            # Receive server response\n","            response_header = client_socket.recv(8)\n","            response_size = int(response_header.decode('utf-8').strip())\n","            response = client_socket.recv(response_size)\n","            server_loss, batch_correct = map(float, response.decode('utf-8').split(','))\n","\n","            # Update statistics\n","            epoch_loss += server_loss\n","            correct_predictions += batch_correct\n","            total_samples += x_batch.shape[0]\n","\n","            # Display batch progress\n","            progress = (batch_index / total_batches) * 100\n","            print(f\"\\rBatch {batch_index}/{total_batches} - Progress: {progress:.2f}% - Loss: {server_loss:.4f}\", end=\"\")\n","\n","        training_accuracy = correct_predictions / total_samples\n","        print(f\"\\nEpoch {epoch + 1} completed: Avg Loss = {epoch_loss / total_batches:.4f}, \"\n","              f\"Training Accuracy = {training_accuracy:.4f}\")\n","\n","    total_training_time = time.time() - start_time\n","    print(f\"\\nTraining completed in {total_training_time:.2f} seconds.\")\n","\n","    # Signal the server to switch to evaluation mode\n","    client_socket.sendall(f\"{-1:08}\".encode('utf-8'))\n","    print(\"Training complete. Signaled server to start evaluation.\")\n","\n","    # Evaluate testing accuracy in batches\n","    print(\"\\nEvaluating on test data...\")\n","    test_batch_size = 512  # Larger batch size for efficient testing\n","    total_test_batches = int(np.ceil(X_test_scaled.shape[0] / test_batch_size))\n","    correct_predictions = 0\n","    total_samples = 0\n","\n","    for i in range(0, X_test_scaled.shape[0], test_batch_size):\n","        x_test_batch = X_test_scaled[i:min(i + test_batch_size, X_test_scaled.shape[0])]\n","        y_test_batch = y_test[i:min(i + test_batch_size, y_test.shape[0])]\n","\n","        test_intermediate = client_model.predict(x_test_batch, verbose=0)\n","        test_data = serialize_data((test_intermediate.tolist(), y_test_batch.tolist()))\n","\n","        client_socket.sendall(f\"{len(test_data):08}\".encode('utf-8'))\n","        client_socket.sendall(test_data)\n","\n","        # Receive test batch accuracy\n","        test_accuracy_response = client_socket.recv(8)\n","        test_accuracy_size = int(test_accuracy_response.decode('utf-8').strip())\n","        test_accuracy_data = client_socket.recv(test_accuracy_size)\n","        batch_correct = int(test_accuracy_data.decode('utf-8'))\n","\n","        correct_predictions += batch_correct\n","        total_samples += x_test_batch.shape[0]\n","\n","    test_accuracy = correct_predictions / total_samples\n","    print(f\"\\nTesting completed: Testing Accuracy = {test_accuracy:.4f}\")\n","\n","    client_socket.close()\n","    print(\"\\nClient connection closed.\")\n","\n","# Start the client\n","start_client()\n"]},{"cell_type":"code","source":[],"metadata":{"id":"QHSiXMHxJ43H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["***Pruned Server-client Models***"],"metadata":{"id":"j07P-UcjfkO_"}},{"cell_type":"code","source":["! pip install -q tensorflow-model-optimization"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LflbwIJ3fa_M","executionInfo":{"status":"ok","timestamp":1746681349721,"user_tz":-330,"elapsed":6216,"user":{"displayName":"V SANTHOSH KUMAR","userId":"08016301954865386445"}},"outputId":"f2769537-ffcf-4712-bac2-76dcde2ba241"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/242.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/18.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/18.3 MB\u001b[0m \u001b[31m243.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m16.7/18.3 MB\u001b[0m \u001b[31m249.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m18.2/18.3 MB\u001b[0m \u001b[31m242.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m122.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import tensorflow_model_optimization as tfmot\n","import tf_keras as keras\n","\n","%load_ext tensorboard\n","\n","import tempfile"],"metadata":{"id":"RWuap7siiUc6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_gzipped_model_size(file):\n","  # Returns size of gzipped model, in bytes.\n","  import os\n","  import zipfile\n","\n","  _, zipped_file = tempfile.mkstemp('.zip')\n","  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n","    f.write(file)\n","\n","  return os.path.getsize(zipped_file)"],"metadata":{"id":"H7ceYDqHh-de"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"pcXgC5jch-mC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wfSZ9p9Wh-4x"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V5oI1yZEhCXz","executionInfo":{"status":"ok","timestamp":1745904243655,"user_tz":-330,"elapsed":462,"user":{"displayName":"V SANTHOSH KUMAR","userId":"08016301954865386445"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ca152e91-2b9b-472c-aebf-0f2efdb9046b"},"outputs":[{"output_type":"stream","name":"stderr","text":["Exception in thread Thread-10 (start_server):\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n"]}],"source":["import socket\n","import pickle\n","import zlib\n","import tensorflow as tf\n","import tf_keras as keras\n","import tensorflow_model_optimization as tfmot\n","import numpy as np\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import MaxPooling1D, Flatten, Dense, BatchNormalization\n","from tensorflow.keras.optimizers import Adam\n","import threading\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","from tensorflow.keras.layers import Dropout\n","# Apply pruning\n","prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n","\n","# Server-side model\n","def create_server_model():\n","    model = keras.Sequential([\n","        keras.layers.MaxPooling1D(pool_size=2,input_shape=(20, 64)),\n","        keras.layers.Conv1D(128, kernel_size=3, activation='relu',padding='same'),\n","        keras.layers.BatchNormalization(),\n","        keras.layers.Conv1D(128, kernel_size=3, activation='relu',padding='same'),\n","        keras.layers.BatchNormalization(),\n","        keras.layers.MaxPooling1D(pool_size=2),\n","        keras.layers.Conv1D(256, kernel_size=3, activation='relu',padding='same'),\n","        keras.layers.BatchNormalization(),\n","        keras.layers.Conv1D(256, kernel_size=3, activation='relu',padding='same'),\n","        keras.layers.BatchNormalization(),\n","        keras.layers.MaxPooling1D(pool_size=2),\n","        keras.layers.Flatten(),\n","        keras.layers.Dense(256, activation='relu'),\n","        keras.layers.Dropout(0.3),\n","        keras.layers.Dense(128, activation='relu'),\n","        keras.layers.Dropout(0.2),\n","        keras.layers.Dense(11, activation='softmax')\n","    ])\n","    return model\n","\n","pruning_params = {\"pruning_schedule\": tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.2, final_sparsity=0.6, begin_step=0, end_step=1000)}\n","server_model = prune_low_magnitude(create_server_model(), **pruning_params)\n","server_model.build(input_shape=(None, 20, 64))\n","server_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Deserialize and decompress data\n","def deserialize_data(data):\n","    return pickle.loads(zlib.decompress(data))\n","\n","# Server function\n","def start_server(host='127.0.0.1', port=65438):\n","\n","    pruned_server_model=server_model\n","    pruning_callback = tfmot.sparsity.keras.UpdatePruningStep()\n","    log_callback = tfmot.sparsity.keras.PruningSummaries(log_dir=\"logs\")\n","\n","\n","    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n","    server_socket.bind((host, port))\n","    server_socket.listen(1)\n","    print(\"Server waiting for connection...\")\n","    conn, addr = server_socket.accept()\n","    print(f\"Connected to client: {addr}\")\n","    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n","    initial_learning_rate=0.001,\n","    decay_steps=10000,\n","    decay_rate=0.9\n","    )\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n","\n","    total_comm_overhead = 0  # Track communication overhead\n","    pruning_callback.set_model(pruned_server_model)\n","    log_callback.set_model(pruned_server_model)\n","    pruning_callback.on_train_begin()\n","\n","    all_true_labels = []\n","    all_pred_labels = []\n","\n","    training_start_time = time.time()\n","\n","\n","    while True:\n","        try:\n","            # Receive header indicating batch size\n","            header = conn.recv(8)\n","            if not header:\n","                break\n","            data_size = int(header.decode('utf-8').strip())\n","\n","\n","            # If a special flag (e.g., -1) is sent, switch to evaluation\n","            if data_size == -1:\n","              print(\"Switching to evaluation phase...\")\n","              break\n","\n","            # Receive the batch data\n","            data = b\"\"\n","            while len(data) < data_size:\n","                packet = conn.recv(4096)\n","                if not packet:\n","                    break\n","                data += packet\n","            total_comm_overhead += len(data)  # Update comm overhead\n","\n","            # Deserialize the data\n","            intermediate_output, labels = deserialize_data(data)\n","            pruning_callback.on_train_batch_begin(batch=-1)\n","\n","            # Perform forward pass and backpropagation on batch\n","            with tf.GradientTape() as tape:\n","                logits = pruned_server_model(np.array(intermediate_output), training=True)\n","                loss = tf.reduce_mean(\n","                    tf.keras.losses.categorical_crossentropy(np.array(labels), logits)\n","                )\n","            gradients = tape.gradient(loss, pruned_server_model.trainable_variables)\n","            optimizer.apply_gradients(zip(gradients, pruned_server_model.trainable_variables))\n","            pruning_callback.on_epoch_end(batch=-1)\n","\n","            pred_classes = np.argmax(logits.numpy(), axis=1)\n","            true_classes = np.argmax(labels, axis=1)\n","\n","            all_pred_labels.extend(pred_classes)\n","            all_true_labels.extend(true_classes)\n","\n","            correct_predictions = int(np.sum(\n","                np.argmax(logits.numpy(), axis=1) == np.argmax(labels, axis=1)\n","            ))\n","            response = f\"{loss.numpy():.4f},{correct_predictions}\"\n","            conn.sendall(f\"{len(response):08}\".encode('utf-8'))\n","            conn.sendall(response.encode('utf-8'))\n","\n","        except Exception as e:\n","            print(f\"Error: {e}\")\n","            break\n","\n","    training_end_time = time.time()\n","    print(f\"Total Training Time: {training_end_time - training_start_time:.2f} sec\")\n","    print(f\"Total Communication Overhead: {total_comm_overhead / 1024:.2f} KB\")\n","\n","\n","    # At the end of training\n","    precision = precision_score(all_true_labels, all_pred_labels, average='macro')\n","    recall = recall_score(all_true_labels, all_pred_labels, average='macro')\n","    f1 = f1_score(all_true_labels, all_pred_labels, average='macro')\n","    print(f\"\\nTraining Metrics → Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n","\n","    # Strip pruning and save the final model\n","    pruned_server_model = tfmot.sparsity.keras.strip_pruning(pruned_server_model)\n","    server_model.save(\"server_model_pruned.h5\")\n","\n","    converter = tf.lite.TFLiteConverter.from_keras_model(pruned_server_model)\n","    tflite_model = converter.convert()\n","    with open(\"server_model_pruned.tflite\", \"wb\") as f:\n","        f.write(tflite_model)\n","\n","    print(\"Model saved in .h5 and .tflite formats.\")\n","\n","    # Handle test evaluation\n","    print(\"Evaluating test data...\")\n","\n","    all_test_true = []\n","    all_test_pred = []\n","\n","    while True:\n","        header = conn.recv(8)\n","        if not header:\n","            break\n","        test_data_size = int(header.decode('utf-8').strip())\n","        test_data = b\"\"\n","        while len(test_data) < test_data_size:\n","            packet = conn.recv(4096)\n","            if not packet:\n","                break\n","            test_data += packet\n","\n","        test_intermediate, test_labels = deserialize_data(test_data)\n","        #logits = pruned_server_model(np.array(test_intermediate), training=False)\n","\n","        test_intermediate = np.array(test_intermediate)\n","        test_labels = np.array(test_labels)\n","\n","        logits = pruned_server_model(test_intermediate, training=False)\n","        pred_classes = np.argmax(logits.numpy(), axis=1)\n","        true_classes = np.argmax(test_labels, axis=1)\n","\n","        all_test_pred.extend(pred_classes)\n","        all_test_true.extend(true_classes)\n","\n","        correct_predictions = int(np.sum(\n","            np.argmax(logits.numpy(), axis=1) == np.argmax(test_labels, axis=1)\n","        ))\n","        response = f\"{correct_predictions}\"\n","        conn.sendall(f\"{len(response):08}\".encode('utf-8'))\n","        conn.sendall(response.encode('utf-8'))\n","\n","        # Final test metrics\n","        precision = precision_score(all_test_true, all_test_pred, average='macro')\n","        recall = recall_score(all_test_true, all_test_pred, average='macro')\n","        f1 = f1_score(all_test_true, all_test_pred, average='macro')\n","        print(f\"\\nTesting Metrics → Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n","\n","\n","    conn.close()\n","    server_socket.close()\n","    print(\"Server connection closed.\")\n","\n","server_thread = threading.Thread(target=start_server)\n","server_thread.start()\n"]},{"cell_type":"code","source":[],"metadata":{"id":"mBYj_V8q9sbv"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ef6ca079-ee44-40ea-d6d5-1d023ebd0714","executionInfo":{"status":"ok","timestamp":1745910381845,"user_tz":-330,"elapsed":3264534,"user":{"displayName":"V SANTHOSH KUMAR","userId":"08016301954865386445"}},"id":"yjQx2FyPhHv9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Connected to server!Connected to client: ('127.0.0.1', 40772)\n","\n","\n","Starting Training...\n","\n","Epoch 1/10\n","Batch 553/553 - Progress: 100.00% - Loss: 0.2483\n","Epoch 1 completed: Avg Loss = 0.3479, Training Accuracy = 0.8188\n","\n","Epoch 2/10\n","Batch 553/553 - Progress: 100.00% - Loss: 0.1789\n","Epoch 2 completed: Avg Loss = 0.2505, Training Accuracy = 0.8550\n","\n","Epoch 3/10\n","Batch 553/553 - Progress: 100.00% - Loss: 0.1523\n","Epoch 3 completed: Avg Loss = 0.1748, Training Accuracy = 0.8903\n","\n","Epoch 4/10\n","Batch 553/553 - Progress: 100.00% - Loss: 0.1281\n","Epoch 4 completed: Avg Loss = 0.1482, Training Accuracy = 0.9015\n","\n","Epoch 5/10\n","Batch 553/553 - Progress: 100.00% - Loss: 0.1243\n","Epoch 5 completed: Avg Loss = 0.1432, Training Accuracy = 0.9028\n","\n","Epoch 6/10\n","Batch 553/553 - Progress: 100.00% - Loss: 0.1237\n","Epoch 6 completed: Avg Loss = 0.1381, Training Accuracy = 0.9049\n","\n","Epoch 7/10\n","Batch 553/553 - Progress: 100.00% - Loss: 0.1229\n","Epoch 7 completed: Avg Loss = 0.1358, Training Accuracy = 0.9060\n","\n","Epoch 8/10\n","Batch 553/553 - Progress: 100.00% - Loss: 0.1243\n","Epoch 8 completed: Avg Loss = 0.1357, Training Accuracy = 0.9053\n","\n","Epoch 9/10\n","Batch 553/553 - Progress: 100.00% - Loss: 0.1210\n","Epoch 9 completed: Avg Loss = 0.1328, Training Accuracy = 0.9060\n","\n","Epoch 10/10\n","Batch 552/553 - Progress: 99.82% - Loss: 0.1323"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-43-248437b05133>:115: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n","  keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"output_type":"stream","name":"stdout","text":["\rBatch 553/553 - Progress: 100.00% - Loss: 0.1228\n","Epoch 10 completed: Avg Loss = 0.1343, Training Accuracy = 0.9070\n","\n","Training completed in 5879.91 seconds.\n","Total Communication Overhead: 12167440.65 KB\n","Saved pruned Keras model to: /tmp/tmpqxyuq9lz.h5\n","Size of gzipped pruned Keras model: 32499.00 bytes\n","Saved pruned TFLite model to: /tmp/tmp21kdbiac.tflite\n","Size of gzipped pruned TFlite model: 31115.00 bytes\n","Training complete. Signaled server to start evaluation.\n","\n","Evaluating on test data...\n","Switching to evaluation phase...\n","Total Training Time: 5882.24 sec\n","Total Communication Overhead: 12167440.65 KB\n","\n","Training Metrics → Precision: 0.8899, Recall: 0.8897, F1 Score: 0.8897\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["Model saved in .h5 and .tflite formats.\n","Evaluating test data...\n","\n","Testing completed: Testing Accuracy = 0.8981\n","\n","Client connection closed.\n"]}],"source":["import socket\n","import pickle\n","import zlib\n","import time\n","import tensorflow as tf\n","import numpy as np\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical\n","\n","# Client-side model\n","def create_client_model():\n","    model =keras.Sequential([\n","        keras.layers.InputLayer(input_shape=(40, 1)),\n","        keras.layers.Conv1D(16, kernel_size=3, activation='relu',padding='same'),\n","        keras.layers.BatchNormalization(),\n","        keras.layers.Conv1D(32, kernel_size=3, activation='relu',padding='same'),\n","        keras.layers.BatchNormalization(),\n","        #MaxPooling1D(pool_size=2),\n","        keras.layers.Conv1D(32, kernel_size=3, activation='relu',padding='same'),\n","        keras.layers.BatchNormalization(),\n","        #MaxPooling1D(pool_size=2),\n","        keras.layers.Conv1D(64, kernel_size=3, activation='relu',padding='same'),\n","        keras.layers.BatchNormalization(),\n","        keras.layers.Conv1D(64, kernel_size=3, activation='relu',padding='same'),\n","        keras.layers.BatchNormalization(),\n","        keras.layers.MaxPooling1D(pool_size=2),\n","\n","    ])\n","    return model\n","\n","    # Apply pruning\n","pruning_params = {'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50, final_sparsity=0.80, begin_step=0, end_step=1000)}\n","pruned_client_model = prune_low_magnitude(create_client_model(), **pruning_params)\n","pruned_client_model.build(input_shape=(None, 40, 1))\n","#print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_client_model)))\n","\n","# Serialize and compress data using pickle\n","def serialize_data(data):\n","    return zlib.compress(pickle.dumps(data))\n","\n","# Client function\n","def start_client(host='127.0.0.1', port=65438):\n","    client_model = create_client_model()\n","    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n","    client_socket.connect((host, port))\n","    print(\"Connected to server!\")\n","\n","    pruning_callback = tfmot.sparsity.keras.UpdatePruningStep()\n","    pruning_callback.set_model(pruned_client_model)\n","    pruning_callback.on_train_begin()\n","\n","    batch_size = 512\n","    epochs =  10\n","    total_batches = int(np.ceil(X_train_scaled.shape[0] / batch_size))\n","    total_comm_overhead = 0\n","\n","    print(\"\\nStarting Training...\")\n","    start_time = time.time()\n","\n","    for epoch in range(epochs):\n","        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n","        epoch_loss = 0\n","        correct_predictions = 0\n","        total_samples = 0\n","\n","        for batch_index, i in enumerate(range(0, X_train_scaled.shape[0], batch_size), start=1):\n","            x_batch = X_train_scaled[i:min(i + batch_size, X_train_scaled.shape[0])]\n","            y_batch = y_train[i:min(i + batch_size, y_train.shape[0])]\n","\n","            pruning_callback.on_train_batch_begin(batch=-1)\n","\n","            # Forward pass to get intermediate activations\n","            intermediate_output = pruned_client_model(x_batch, training=True).numpy()\n","\n","            # Serialize and send data\n","            data = serialize_data((intermediate_output.tolist(), y_batch.tolist()))\n","            data_size = len(data)\n","\n","            client_socket.sendall(f\"{len(data):08}\".encode('utf-8'))\n","            client_socket.sendall(data)\n","\n","            total_comm_overhead += data_size\n","\n","            # Receive server response\n","            response_header = client_socket.recv(8)\n","            response_size = int(response_header.decode('utf-8').strip())\n","            response = client_socket.recv(response_size)\n","            server_loss, batch_correct = map(float, response.decode('utf-8').split(','))\n","            pruning_callback.on_epoch_end(batch=-1)\n","\n","            # Update statistics\n","            epoch_loss += server_loss\n","            correct_predictions += batch_correct\n","            total_samples += x_batch.shape[0]\n","\n","            # Display batch progress\n","            progress = (batch_index / total_batches) * 100\n","            print(f\"\\rBatch {batch_index}/{total_batches} - Progress: {progress:.2f}% - Loss: {server_loss:.4f}\", end=\"\")\n","\n","        training_accuracy = correct_predictions / total_samples\n","        print(f\"\\nEpoch {epoch + 1} completed: Avg Loss = {epoch_loss / total_batches:.4f}, \"\n","              f\"Training Accuracy = {training_accuracy:.4f}\")\n","\n","    total_training_time = time.time() - start_time\n","    print(f\"\\nTraining completed in {total_training_time:.2f} seconds.\")\n","    print(f\"Total Communication Overhead: {total_comm_overhead / 1024:.2f} KB\")\n","\n","    #client_model = tfmot.sparsity.keras.strip_pruning(pruned_client_model)\n","    #client_model.save(\"client_model_pruned.h5\")\n","    model_for_export = tfmot.sparsity.keras.strip_pruning(pruned_client_model)\n","\n","    _, pruned_keras_file = tempfile.mkstemp('.h5')\n","    keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n","    print('Saved pruned Keras model to:', pruned_keras_file)\n","\n","    print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\n","\n","    #converter = tf.lite.TFLiteConverter.from_keras_model(pruned_client_model)\n","    #tflite_model = converter.convert()\n","    #with open(\"client_model_pruned.tflite\", \"wb\") as f:\n","        #f.write(tflite_model)\n","    converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n","    pruned_tflite_model = converter.convert()\n","\n","    _, pruned_tflite_file = tempfile.mkstemp('.tflite')\n","\n","    with open(pruned_tflite_file, 'wb') as f:\n","        f.write(pruned_tflite_model)\n","\n","    print('Saved pruned TFLite model to:', pruned_tflite_file)\n","    print(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(pruned_tflite_file)))\n","\n","        #print(\"Client model saved in .h5 and .tflite formats.\")\n","    # Signal the server to switch to evaluation mode\n","    client_socket.sendall(f\"{-1:08}\".encode('utf-8'))\n","    print(\"Training complete. Signaled server to start evaluation.\")\n","\n","    # Evaluate testing accuracy in batches\n","    print(\"\\nEvaluating on test data...\")\n","    test_batch_size = 512  # Larger batch size for efficient testing\n","    total_test_batches = int(np.ceil(X_test_scaled.shape[0] / test_batch_size))\n","    correct_predictions = 0\n","    total_samples = 0\n","\n","    for i in range(0, X_test_scaled.shape[0], test_batch_size):\n","        x_test_batch = X_test_scaled[i:min(i + test_batch_size, X_test_scaled.shape[0])]\n","        y_test_batch = y_test[i:min(i + test_batch_size, y_test.shape[0])]\n","\n","        test_intermediate = pruned_client_model.predict(x_test_batch, verbose=0)\n","        test_data = serialize_data((test_intermediate.tolist(), y_test_batch.tolist()))\n","\n","        client_socket.sendall(f\"{len(test_data):08}\".encode('utf-8'))\n","        client_socket.sendall(test_data)\n","\n","        # Receive test batch accuracy\n","        test_accuracy_response = client_socket.recv(8)\n","        test_accuracy_size = int(test_accuracy_response.decode('utf-8').strip())\n","        test_accuracy_data = client_socket.recv(test_accuracy_size)\n","        batch_correct = int(test_accuracy_data.decode('utf-8'))\n","\n","        correct_predictions += batch_correct\n","        total_samples += x_test_batch.shape[0]\n","\n","    test_accuracy = correct_predictions / total_samples\n","    print(f\"\\nTesting completed: Testing Accuracy = {test_accuracy:.4f}\")\n","\n","    client_socket.close()\n","    print(\"\\nClient connection closed.\")\n","\n","# Start the client\n","start_client()\n"]},{"cell_type":"markdown","source":["***With Optimized Pruning Parameters***"],"metadata":{"id":"T-LvCGvsIVOl"}},{"cell_type":"code","source":[],"metadata":{"id":"B-r3hIPxfbFP"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z1ZERugiInyM"},"outputs":[],"source":["import socket\n","import pickle\n","import zlib\n","import tensorflow as tf\n","import tf_keras as keras\n","import tensorflow_model_optimization as tfmot\n","import numpy as np\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import MaxPooling1D, Flatten, Dense, BatchNormalization\n","from tensorflow.keras.optimizers import Adam\n","import threading\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","from tensorflow.keras.layers import Dropout\n","# Apply pruning\n","prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n","\n","# Server-side model\n","def create_server_model():\n","    model = keras.Sequential([\n","        keras.layers.MaxPooling1D(pool_size=2,input_shape=(20, 64)),\n","        keras.layers.Conv1D(128, kernel_size=3, activation='relu',padding='same'),\n","        keras.layers.BatchNormalization(),\n","        keras.layers.Conv1D(128, kernel_size=3, activation='relu',padding='same'),\n","        keras.layers.BatchNormalization(),\n","        keras.layers.MaxPooling1D(pool_size=2),\n","        keras.layers.Conv1D(256, kernel_size=3, activation='relu',padding='same'),\n","        keras.layers.BatchNormalization(),\n","        keras.layers.Conv1D(256, kernel_size=3, activation='relu',padding='same'),\n","        keras.layers.BatchNormalization(),\n","        keras.layers.MaxPooling1D(pool_size=2),\n","        keras.layers.Flatten(),\n","        keras.layers.Dense(256, activation='relu'),\n","        keras.layers.Dropout(0.3),\n","        keras.layers.Dense(128, activation='relu'),\n","        keras.layers.Dropout(0.2),\n","        keras.layers.Dense(11, activation='softmax')\n","    ])\n","    return model\n","\n","pruning_params = {\"pruning_schedule\": tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.0, final_sparsity=0.6, begin_step=0, end_step=5530, power=3)}\n","server_model = prune_low_magnitude(create_server_model(), **pruning_params)\n","server_model.build(input_shape=(None, 20, 64))\n","server_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Deserialize and decompress data\n","def deserialize_data(data):\n","    return pickle.loads(zlib.decompress(data))\n","\n","# Server function\n","def start_server(host='127.0.0.1', port=65438):\n","\n","    pruned_server_model=server_model\n","    pruning_callback = tfmot.sparsity.keras.UpdatePruningStep()\n","    log_callback = tfmot.sparsity.keras.PruningSummaries(log_dir=\"logs\")\n","\n","\n","    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n","    server_socket.bind((host, port))\n","    server_socket.listen(1)\n","    print(\"Server waiting for connection...\")\n","    conn, addr = server_socket.accept()\n","    print(f\"Connected to client: {addr}\")\n","    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n","    initial_learning_rate=0.001,\n","    decay_steps=10000,\n","    decay_rate=0.9\n","    )\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n","\n","    total_comm_overhead = 0  # Track communication overhead\n","    pruning_callback.set_model(pruned_server_model)\n","    log_callback.set_model(pruned_server_model)\n","    pruning_callback.on_train_begin()\n","\n","    all_true_labels = []\n","    all_pred_labels = []\n","\n","    training_start_time = time.time()\n","\n","\n","    while True:\n","        try:\n","            # Receive header indicating batch size\n","            header = conn.recv(8)\n","            if not header:\n","                break\n","            data_size = int(header.decode('utf-8').strip())\n","\n","\n","            # If a special flag (e.g., -1) is sent, switch to evaluation\n","            if data_size == -1:\n","              print(\"Switching to evaluation phase...\")\n","              break\n","\n","            # Receive the batch data\n","            data = b\"\"\n","            while len(data) < data_size:\n","                packet = conn.recv(4096)\n","                if not packet:\n","                    break\n","                data += packet\n","            total_comm_overhead += len(data)  # Update comm overhead\n","\n","            # Deserialize the data\n","            intermediate_output, labels = deserialize_data(data)\n","            pruning_callback.on_train_batch_begin(batch=-1)\n","\n","            # Perform forward pass and backpropagation on batch\n","            with tf.GradientTape() as tape:\n","                logits = pruned_server_model(np.array(intermediate_output), training=True)\n","                loss = tf.reduce_mean(\n","                    tf.keras.losses.categorical_crossentropy(np.array(labels), logits)\n","                )\n","            gradients = tape.gradient(loss, pruned_server_model.trainable_variables)\n","            optimizer.apply_gradients(zip(gradients, pruned_server_model.trainable_variables))\n","            pruning_callback.on_epoch_end(batch=-1)\n","\n","            pred_classes = np.argmax(logits.numpy(), axis=1)\n","            true_classes = np.argmax(labels, axis=1)\n","\n","            all_pred_labels.extend(pred_classes)\n","            all_true_labels.extend(true_classes)\n","\n","            correct_predictions = int(np.sum(\n","                np.argmax(logits.numpy(), axis=1) == np.argmax(labels, axis=1)\n","            ))\n","            response = f\"{loss.numpy():.4f},{correct_predictions}\"\n","            conn.sendall(f\"{len(response):08}\".encode('utf-8'))\n","            conn.sendall(response.encode('utf-8'))\n","\n","        except Exception as e:\n","            print(f\"Error: {e}\")\n","            break\n","\n","    training_end_time = time.time()\n","    print(f\"Total Training Time: {training_end_time - training_start_time:.2f} sec\")\n","    print(f\"Total Communication Overhead: {total_comm_overhead / 1024:.2f} KB\")\n","\n","\n","    # At the end of training\n","    precision = precision_score(all_true_labels, all_pred_labels, average='macro')\n","    recall = recall_score(all_true_labels, all_pred_labels, average='macro')\n","    f1 = f1_score(all_true_labels, all_pred_labels, average='macro')\n","    print(f\"\\nTraining Metrics → Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n","\n","    # Strip pruning and save the final model\n","    pruned_server_model = tfmot.sparsity.keras.strip_pruning(pruned_server_model)\n","    server_model.save(\"server_model_pruned.h5\")\n","\n","    converter = tf.lite.TFLiteConverter.from_keras_model(pruned_server_model)\n","    tflite_model = converter.convert()\n","    with open(\"server_model_pruned.tflite\", \"wb\") as f:\n","        f.write(tflite_model)\n","\n","    print(\"Model saved in .h5 and .tflite formats.\")\n","\n","    # Handle test evaluation\n","    print(\"Evaluating test data...\")\n","\n","    all_test_true = []\n","    all_test_pred = []\n","\n","    while True:\n","        header = conn.recv(8)\n","        if not header:\n","            break\n","        test_data_size = int(header.decode('utf-8').strip())\n","        test_data = b\"\"\n","        while len(test_data) < test_data_size:\n","            packet = conn.recv(4096)\n","            if not packet:\n","                break\n","            test_data += packet\n","\n","        test_intermediate, test_labels = deserialize_data(test_data)\n","        #logits = pruned_server_model(np.array(test_intermediate), training=False)\n","\n","        test_intermediate = np.array(test_intermediate)\n","        test_labels = np.array(test_labels)\n","\n","        logits = pruned_server_model(test_intermediate, training=False)\n","        pred_classes = np.argmax(logits.numpy(), axis=1)\n","        true_classes = np.argmax(test_labels, axis=1)\n","\n","        all_test_pred.extend(pred_classes)\n","        all_test_true.extend(true_classes)\n","\n","        correct_predictions = int(np.sum(\n","            np.argmax(logits.numpy(), axis=1) == np.argmax(test_labels, axis=1)\n","        ))\n","        response = f\"{correct_predictions}\"\n","        conn.sendall(f\"{len(response):08}\".encode('utf-8'))\n","        conn.sendall(response.encode('utf-8'))\n","\n","        # Final test metrics\n","        precision = precision_score(all_test_true, all_test_pred, average='macro')\n","        recall = recall_score(all_test_true, all_test_pred, average='macro')\n","        f1 = f1_score(all_test_true, all_test_pred, average='macro')\n","        print(f\"\\nTesting Metrics → Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n","\n","\n","    conn.close()\n","    server_socket.close()\n","    print(\"Server connection closed.\")\n","\n","server_thread = threading.Thread(target=start_server)\n","server_thread.start()\n"]},{"cell_type":"code","source":[],"metadata":{"id":"dgZZnHOtfbIS"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d63be9d5-2277-46e7-9783-16cbb66998e3","id":"EKb_zv6SIs4T","executionInfo":{"status":"ok","timestamp":1746080337774,"user_tz":-330,"elapsed":6236214,"user":{"displayName":"V SANTHOSH KUMAR","userId":"08016301954865386445"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Connected to server!Connected to client: ('127.0.0.1', 41396)\n","\n","\n","Starting Training...\n","\n","Epoch 1/10\n","Batch 553/553 - Progress: 100.00% - Loss: 0.2147\n","Epoch 1 completed: Avg Loss = 0.2589, Training Accuracy = 0.8539\n","\n","Epoch 2/10\n","Batch 553/553 - Progress: 100.00% - Loss: 0.2566\n","Epoch 2 completed: Avg Loss = 0.1966, Training Accuracy = 0.8825\n","\n","Epoch 3/10\n","Batch 553/553 - Progress: 100.00% - Loss: 0.1660\n","Epoch 3 completed: Avg Loss = 0.2013, Training Accuracy = 0.8811\n","\n","Epoch 4/10\n","Batch 553/553 - Progress: 100.00% - Loss: 0.1886\n","Epoch 4 completed: Avg Loss = 0.2061, Training Accuracy = 0.8787\n","\n","Epoch 5/10\n","Batch 553/553 - Progress: 100.00% - Loss: 0.2241\n","Epoch 5 completed: Avg Loss = 0.2199, Training Accuracy = 0.8701\n","\n","Epoch 6/10\n","Batch 553/553 - Progress: 100.00% - Loss: 0.2055\n","Epoch 6 completed: Avg Loss = 0.2351, Training Accuracy = 0.8606\n","\n","Epoch 7/10\n","Batch 553/553 - Progress: 100.00% - Loss: 0.1752\n","Epoch 7 completed: Avg Loss = 0.2173, Training Accuracy = 0.8691\n","\n","Epoch 8/10\n","Batch 553/553 - Progress: 100.00% - Loss: 0.2304\n","Epoch 8 completed: Avg Loss = 0.1906, Training Accuracy = 0.8839\n","\n","Epoch 9/10\n","Batch 553/553 - Progress: 100.00% - Loss: 0.1541\n","Epoch 9 completed: Avg Loss = 0.1867, Training Accuracy = 0.8842\n","\n","Epoch 10/10\n","Batch 552/553 - Progress: 99.82% - Loss: 0.1484"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-41-bb187517b709>:115: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n","  keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"output_type":"stream","name":"stdout","text":["\rBatch 553/553 - Progress: 100.00% - Loss: 0.1355\n","Epoch 10 completed: Avg Loss = 0.1568, Training Accuracy = 0.8970\n","\n","Training completed in 5956.90 seconds.\n","Total Communication Overhead: 12340109.82 KB\n","Saved pruned Keras model to: /tmp/tmpn95jo1gu.h5\n","Size of gzipped pruned Keras model: 49038.00 bytes\n","Saved pruned TFLite model to: /tmp/tmpv2enu9ka.tflite\n","Size of gzipped pruned TFlite model: 47708.00 bytes\n","Training complete. Signaled server to start evaluation.\n","\n","Evaluating on test data...\n","Switching to evaluation phase...\n","Total Training Time: 5958.81 sec\n","Total Communication Overhead: 12340109.82 KB\n","\n","Training Metrics → Precision: 0.8764, Recall: 0.8760, F1 Score: 0.8759\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["Model saved in .h5 and .tflite formats.\n","Evaluating test data...\n","\n","Testing Metrics → Precision: 0.8187, Recall: 0.8583, F1 Score: 0.8199\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"output_type":"stream","name":"stdout","text":["\n","Testing Metrics → Precision: 0.8294, Recall: 0.8592, F1 Score: 0.8263\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"output_type":"stream","name":"stdout","text":["\n","Testing Metrics → Precision: 0.8275, Recall: 0.8601, F1 Score: 0.8246\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"output_type":"stream","name":"stdout","text":["\n","Testing Metrics → Precision: 0.8258, Recall: 0.8580, F1 Score: 0.8207\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"output_type":"stream","name":"stdout","text":["\n","Testing Metrics → Precision: 0.8250, Recall: 0.8577, F1 Score: 0.8204\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"output_type":"stream","name":"stdout","text":["\n","Testing Metrics → Precision: 0.8261, Recall: 0.8585, F1 Score: 0.8216\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"output_type":"stream","name":"stdout","text":["\n","Testing Metrics → Precision: 0.8262, Recall: 0.8584, F1 Score: 0.8220\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"output_type":"stream","name":"stdout","text":["\n","Testing Metrics → Precision: 0.8259, Recall: 0.8586, F1 Score: 0.8217\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"output_type":"stream","name":"stdout","text":["\n","Testing Metrics → Precision: 0.8277, Recall: 0.8589, F1 Score: 0.8233\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"output_type":"stream","name":"stdout","text":["\n","Testing Metrics → Precision: 0.8278, Recall: 0.8595, F1 Score: 0.8238\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"output_type":"stream","name":"stdout","text":["\n","Testing Metrics → Precision: 0.8272, Recall: 0.8590, F1 Score: 0.8230\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"output_type":"stream","name":"stdout","text":["\n","Testing Metrics → Precision: 0.8267, Recall: 0.8588, F1 Score: 0.8226\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"output_type":"stream","name":"stdout","text":["\n","Testing Metrics → Precision: 0.8262, Recall: 0.8586, F1 Score: 0.8225\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"output_type":"stream","name":"stdout","text":["\n","Testing Metrics → Precision: 0.8268, Recall: 0.8587, F1 Score: 0.8231\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"output_type":"stream","name":"stdout","text":["\n","Testing Metrics → Precision: 0.8271, Recall: 0.8596, F1 Score: 0.8239\n","\n","Testing Metrics → Precision: 0.9180, Recall: 0.8596, F1 Score: 0.8240\n","\n","Testing Metrics → Precision: 0.9183, Recall: 0.8599, F1 Score: 0.8245\n","\n","Testing Metrics → Precision: 0.9185, Recall: 0.8599, F1 Score: 0.8248\n","\n","Testing Metrics → Precision: 0.9194, Recall: 0.8602, F1 Score: 0.8259\n","\n","Testing Metrics → Precision: 0.9189, Recall: 0.8604, F1 Score: 0.8256\n","\n","Testing Metrics → Precision: 0.9185, Recall: 0.8609, F1 Score: 0.8256\n","\n","Testing Metrics → Precision: 0.9190, Recall: 0.8611, F1 Score: 0.8262\n","\n","Testing Metrics → Precision: 0.9186, Recall: 0.8609, F1 Score: 0.8258\n","\n","Testing Metrics → Precision: 0.9185, Recall: 0.8609, F1 Score: 0.8257\n","\n","Testing Metrics → Precision: 0.9186, Recall: 0.8606, F1 Score: 0.8254\n","\n","Testing Metrics → Precision: 0.9186, Recall: 0.8604, F1 Score: 0.8252\n","\n","Testing Metrics → Precision: 0.9187, Recall: 0.8604, F1 Score: 0.8253\n","\n","Testing Metrics → Precision: 0.9186, Recall: 0.8603, F1 Score: 0.8252\n","\n","Testing Metrics → Precision: 0.9189, Recall: 0.8604, F1 Score: 0.8254\n","\n","Testing Metrics → Precision: 0.9187, Recall: 0.8604, F1 Score: 0.8254\n","\n","Testing Metrics → Precision: 0.9189, Recall: 0.8606, F1 Score: 0.8256\n","\n","Testing Metrics → Precision: 0.9188, Recall: 0.8607, F1 Score: 0.8256\n","\n","Testing Metrics → Precision: 0.9190, Recall: 0.8605, F1 Score: 0.8257\n","\n","Testing Metrics → Precision: 0.9190, Recall: 0.8605, F1 Score: 0.8257\n","\n","Testing Metrics → Precision: 0.9194, Recall: 0.8606, F1 Score: 0.8260\n","\n","Testing Metrics → Precision: 0.9193, Recall: 0.8605, F1 Score: 0.8259\n","\n","Testing Metrics → Precision: 0.9194, Recall: 0.8604, F1 Score: 0.8260\n","\n","Testing Metrics → Precision: 0.9192, Recall: 0.8603, F1 Score: 0.8257\n","\n","Testing Metrics → Precision: 0.9190, Recall: 0.8603, F1 Score: 0.8257\n","\n","Testing Metrics → Precision: 0.9191, Recall: 0.8606, F1 Score: 0.8259\n","\n","Testing Metrics → Precision: 0.9191, Recall: 0.8606, F1 Score: 0.8260\n","\n","Testing Metrics → Precision: 0.9191, Recall: 0.8610, F1 Score: 0.8263\n","\n","Testing Metrics → Precision: 0.9190, Recall: 0.8611, F1 Score: 0.8262\n","\n","Testing Metrics → Precision: 0.9190, Recall: 0.8612, F1 Score: 0.8263\n","\n","Testing Metrics → Precision: 0.9189, Recall: 0.8612, F1 Score: 0.8263\n","\n","Testing Metrics → Precision: 0.9191, Recall: 0.8613, F1 Score: 0.8264\n","\n","Testing Metrics → Precision: 0.9192, Recall: 0.8614, F1 Score: 0.8266\n","\n","Testing Metrics → Precision: 0.9191, Recall: 0.8614, F1 Score: 0.8266\n","\n","Testing Metrics → Precision: 0.9192, Recall: 0.8614, F1 Score: 0.8265\n","\n","Testing Metrics → Precision: 0.9192, Recall: 0.8614, F1 Score: 0.8265\n","\n","Testing Metrics → Precision: 0.9192, Recall: 0.8612, F1 Score: 0.8263\n","\n","Testing Metrics → Precision: 0.9193, Recall: 0.8612, F1 Score: 0.8264\n","\n","Testing Metrics → Precision: 0.9196, Recall: 0.8612, F1 Score: 0.8267\n","\n","Testing Metrics → Precision: 0.9197, Recall: 0.8616, F1 Score: 0.8270\n","\n","Testing Metrics → Precision: 0.9197, Recall: 0.8615, F1 Score: 0.8270\n","\n","Testing Metrics → Precision: 0.9197, Recall: 0.8616, F1 Score: 0.8270\n","\n","Testing Metrics → Precision: 0.9197, Recall: 0.8615, F1 Score: 0.8269\n","\n","Testing Metrics → Precision: 0.9198, Recall: 0.8615, F1 Score: 0.8270\n","\n","Testing Metrics → Precision: 0.9199, Recall: 0.8615, F1 Score: 0.8271\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8616, F1 Score: 0.8273\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8618, F1 Score: 0.8274\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8618, F1 Score: 0.8274\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8618, F1 Score: 0.8274\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8618, F1 Score: 0.8274\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8617, F1 Score: 0.8275\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8616, F1 Score: 0.8273\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8616, F1 Score: 0.8274\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8617, F1 Score: 0.8275\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8618, F1 Score: 0.8277\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8617, F1 Score: 0.8275\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8618, F1 Score: 0.8276\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8618, F1 Score: 0.8275\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8618, F1 Score: 0.8275\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8618, F1 Score: 0.8276\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8619, F1 Score: 0.8277\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8618, F1 Score: 0.8276\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8618, F1 Score: 0.8276\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8619, F1 Score: 0.8277\n","\n","Testing Metrics → Precision: 0.9199, Recall: 0.8620, F1 Score: 0.8277\n","\n","Testing Metrics → Precision: 0.9198, Recall: 0.8618, F1 Score: 0.8274\n","\n","Testing Metrics → Precision: 0.9198, Recall: 0.8618, F1 Score: 0.8275\n","\n","Testing Metrics → Precision: 0.9198, Recall: 0.8619, F1 Score: 0.8275\n","\n","Testing Metrics → Precision: 0.9197, Recall: 0.8618, F1 Score: 0.8274\n","\n","Testing Metrics → Precision: 0.9197, Recall: 0.8618, F1 Score: 0.8274\n","\n","Testing Metrics → Precision: 0.9198, Recall: 0.8618, F1 Score: 0.8275\n","\n","Testing Metrics → Precision: 0.9199, Recall: 0.8618, F1 Score: 0.8276\n","\n","Testing Metrics → Precision: 0.9199, Recall: 0.8617, F1 Score: 0.8276\n","\n","Testing Metrics → Precision: 0.9198, Recall: 0.8617, F1 Score: 0.8275\n","\n","Testing Metrics → Precision: 0.9198, Recall: 0.8616, F1 Score: 0.8274\n","\n","Testing Metrics → Precision: 0.9198, Recall: 0.8617, F1 Score: 0.8275\n","\n","Testing Metrics → Precision: 0.9197, Recall: 0.8616, F1 Score: 0.8273\n","\n","Testing Metrics → Precision: 0.9198, Recall: 0.8616, F1 Score: 0.8274\n","\n","Testing Metrics → Precision: 0.9197, Recall: 0.8615, F1 Score: 0.8273\n","\n","Testing Metrics → Precision: 0.9197, Recall: 0.8615, F1 Score: 0.8273\n","\n","Testing Metrics → Precision: 0.9196, Recall: 0.8614, F1 Score: 0.8272\n","\n","Testing Metrics → Precision: 0.9195, Recall: 0.8613, F1 Score: 0.8270\n","\n","Testing Metrics → Precision: 0.9195, Recall: 0.8613, F1 Score: 0.8270\n","\n","Testing Metrics → Precision: 0.9195, Recall: 0.8614, F1 Score: 0.8272\n","\n","Testing Metrics → Precision: 0.9196, Recall: 0.8614, F1 Score: 0.8272\n","\n","Testing Metrics → Precision: 0.9196, Recall: 0.8615, F1 Score: 0.8273\n","\n","Testing Metrics → Precision: 0.9197, Recall: 0.8616, F1 Score: 0.8274\n","\n","Testing Metrics → Precision: 0.9196, Recall: 0.8616, F1 Score: 0.8274\n","\n","Testing Metrics → Precision: 0.9196, Recall: 0.8617, F1 Score: 0.8275\n","\n","Testing Metrics → Precision: 0.9197, Recall: 0.8617, F1 Score: 0.8276\n","\n","Testing Metrics → Precision: 0.9197, Recall: 0.8617, F1 Score: 0.8276\n","\n","Testing Metrics → Precision: 0.9199, Recall: 0.8618, F1 Score: 0.8278\n","\n","Testing Metrics → Precision: 0.9198, Recall: 0.8618, F1 Score: 0.8278\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8619, F1 Score: 0.8280\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8620, F1 Score: 0.8282\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8619, F1 Score: 0.8281\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8619, F1 Score: 0.8282\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8619, F1 Score: 0.8282\n","\n","Testing Metrics → Precision: 0.9203, Recall: 0.8619, F1 Score: 0.8282\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8619, F1 Score: 0.8282\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8619, F1 Score: 0.8282\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8620, F1 Score: 0.8283\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8620, F1 Score: 0.8283\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8620, F1 Score: 0.8283\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8621, F1 Score: 0.8283\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8620, F1 Score: 0.8282\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8620, F1 Score: 0.8282\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8620, F1 Score: 0.8282\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8620, F1 Score: 0.8282\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8621, F1 Score: 0.8283\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8621, F1 Score: 0.8283\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8620, F1 Score: 0.8282\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8620, F1 Score: 0.8282\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8620, F1 Score: 0.8281\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8620, F1 Score: 0.8281\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8619, F1 Score: 0.8281\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8620, F1 Score: 0.8281\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8620, F1 Score: 0.8282\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8621, F1 Score: 0.8282\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8621, F1 Score: 0.8283\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8621, F1 Score: 0.8283\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8621, F1 Score: 0.8282\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8621, F1 Score: 0.8283\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8622, F1 Score: 0.8283\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8621, F1 Score: 0.8282\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8621, F1 Score: 0.8282\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8622, F1 Score: 0.8283\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8621, F1 Score: 0.8282\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8621, F1 Score: 0.8282\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8621, F1 Score: 0.8282\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8621, F1 Score: 0.8282\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8621, F1 Score: 0.8282\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8621, F1 Score: 0.8282\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8620, F1 Score: 0.8282\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8620, F1 Score: 0.8281\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8620, F1 Score: 0.8281\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8620, F1 Score: 0.8282\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8621, F1 Score: 0.8283\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8621, F1 Score: 0.8283\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8620, F1 Score: 0.8282\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8620, F1 Score: 0.8281\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8621, F1 Score: 0.8282\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8622, F1 Score: 0.8282\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8622, F1 Score: 0.8282\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8622, F1 Score: 0.8283\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8622, F1 Score: 0.8283\n","\n","Testing Metrics → Precision: 0.9200, Recall: 0.8622, F1 Score: 0.8283\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8621, F1 Score: 0.8283\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8622, F1 Score: 0.8283\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8621, F1 Score: 0.8284\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8621, F1 Score: 0.8283\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8621, F1 Score: 0.8283\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8621, F1 Score: 0.8283\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8621, F1 Score: 0.8284\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8621, F1 Score: 0.8284\n","\n","Testing Metrics → Precision: 0.9203, Recall: 0.8622, F1 Score: 0.8285\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8621, F1 Score: 0.8284\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8622, F1 Score: 0.8285\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8622, F1 Score: 0.8284\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8622, F1 Score: 0.8285\n","\n","Testing Metrics → Precision: 0.9203, Recall: 0.8623, F1 Score: 0.8286\n","\n","Testing Metrics → Precision: 0.9203, Recall: 0.8623, F1 Score: 0.8287\n","\n","Testing Metrics → Precision: 0.9203, Recall: 0.8622, F1 Score: 0.8286\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8622, F1 Score: 0.8285\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8622, F1 Score: 0.8285\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8622, F1 Score: 0.8285\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8622, F1 Score: 0.8285\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8622, F1 Score: 0.8285\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8622, F1 Score: 0.8285\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8622, F1 Score: 0.8285\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8622, F1 Score: 0.8286\n","\n","Testing Metrics → Precision: 0.9203, Recall: 0.8623, F1 Score: 0.8287\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8623, F1 Score: 0.8286\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8623, F1 Score: 0.8286\n","\n","Testing Metrics → Precision: 0.9203, Recall: 0.8623, F1 Score: 0.8287\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8623, F1 Score: 0.8286\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8624, F1 Score: 0.8286\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8624, F1 Score: 0.8286\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8624, F1 Score: 0.8286\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8624, F1 Score: 0.8286\n","\n","Testing Metrics → Precision: 0.9201, Recall: 0.8624, F1 Score: 0.8286\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8624, F1 Score: 0.8287\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8624, F1 Score: 0.8287\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8624, F1 Score: 0.8287\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8624, F1 Score: 0.8287\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8624, F1 Score: 0.8287\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8624, F1 Score: 0.8287\n","\n","Testing Metrics → Precision: 0.9203, Recall: 0.8624, F1 Score: 0.8287\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8624, F1 Score: 0.8287\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8624, F1 Score: 0.8287\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8624, F1 Score: 0.8287\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8624, F1 Score: 0.8287\n","\n","Testing Metrics → Precision: 0.9202, Recall: 0.8624, F1 Score: 0.8287\n","\n","Testing Metrics → Precision: 0.9203, Recall: 0.8624, F1 Score: 0.8288\n","\n","Testing Metrics → Precision: 0.9203, Recall: 0.8625, F1 Score: 0.8288\n","\n","Testing Metrics → Precision: 0.9203, Recall: 0.8625, F1 Score: 0.8288\n","\n","Testing Metrics → Precision: 0.9203, Recall: 0.8624, F1 Score: 0.8288\n","\n","Testing Metrics → Precision: 0.9204, Recall: 0.8625, F1 Score: 0.8289\n","\n","Testing Metrics → Precision: 0.9204, Recall: 0.8624, F1 Score: 0.8289\n","\n","Testing Metrics → Precision: 0.9204, Recall: 0.8624, F1 Score: 0.8289\n","\n","Testing Metrics → Precision: 0.9204, Recall: 0.8625, F1 Score: 0.8289\n","\n","Testing Metrics → Precision: 0.9205, Recall: 0.8625, F1 Score: 0.8289\n","\n","Testing Metrics → Precision: 0.9204, Recall: 0.8625, F1 Score: 0.8289\n","\n","Testing Metrics → Precision: 0.9205, Recall: 0.8625, F1 Score: 0.8290\n","\n","Testing Metrics → Precision: 0.9206, Recall: 0.8625, F1 Score: 0.8290\n","\n","Testing Metrics → Precision: 0.9206, Recall: 0.8625, F1 Score: 0.8290\n","\n","Testing Metrics → Precision: 0.9206, Recall: 0.8625, F1 Score: 0.8290\n","\n","Testing Metrics → Precision: 0.9206, Recall: 0.8625, F1 Score: 0.8290\n","\n","Testing Metrics → Precision: 0.9206, Recall: 0.8625, F1 Score: 0.8290\n","\n","Testing Metrics → Precision: 0.9206, Recall: 0.8625, F1 Score: 0.8290\n","\n","Testing Metrics → Precision: 0.9205, Recall: 0.8625, F1 Score: 0.8290\n","\n","Testing Metrics → Precision: 0.9206, Recall: 0.8626, F1 Score: 0.8290\n","\n","Testing Metrics → Precision: 0.9205, Recall: 0.8626, F1 Score: 0.8290\n","\n","Testing Metrics → Precision: 0.9206, Recall: 0.8626, F1 Score: 0.8291\n","\n","Testing Metrics → Precision: 0.9207, Recall: 0.8626, F1 Score: 0.8292\n","\n","Testing Metrics → Precision: 0.9206, Recall: 0.8625, F1 Score: 0.8291\n","\n","Testing Metrics → Precision: 0.9206, Recall: 0.8625, F1 Score: 0.8291\n","\n","Testing Metrics → Precision: 0.9207, Recall: 0.8625, F1 Score: 0.8291\n","\n","Testing Metrics → Precision: 0.9206, Recall: 0.8624, F1 Score: 0.8290\n","\n","Testing Metrics → Precision: 0.9206, Recall: 0.8624, F1 Score: 0.8290\n","\n","Testing Metrics → Precision: 0.9205, Recall: 0.8624, F1 Score: 0.8289\n","\n","Testing Metrics → Precision: 0.9206, Recall: 0.8624, F1 Score: 0.8290\n","\n","Testing completed: Testing Accuracy = 0.8617\n","\n","Client connection closed.\n"]}],"source":["import socket\n","import pickle\n","import zlib\n","import time\n","import tensorflow as tf\n","import numpy as np\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical\n","\n","# Client-side model\n","def create_client_model():\n","    model =keras.Sequential([\n","        keras.layers.InputLayer(input_shape=(40, 1)),\n","        keras.layers.Conv1D(16, kernel_size=3, activation='relu',padding='same'),\n","        keras.layers.BatchNormalization(),\n","        keras.layers.Conv1D(32, kernel_size=3, activation='relu',padding='same'),\n","        keras.layers.BatchNormalization(),\n","        #MaxPooling1D(pool_size=2),\n","        keras.layers.Conv1D(32, kernel_size=3, activation='relu',padding='same'),\n","        keras.layers.BatchNormalization(),\n","        #MaxPooling1D(pool_size=2),\n","        keras.layers.Conv1D(64, kernel_size=3, activation='relu',padding='same'),\n","        keras.layers.BatchNormalization(),\n","        keras.layers.Conv1D(64, kernel_size=3, activation='relu',padding='same'),\n","        keras.layers.BatchNormalization(),\n","        keras.layers.MaxPooling1D(pool_size=2),\n","\n","    ])\n","    return model\n","\n","    # Apply pruning\n","pruning_params = {'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.0, final_sparsity=0.60, begin_step=0, end_step=5530,power=3)}\n","pruned_client_model = prune_low_magnitude(create_client_model(), **pruning_params)\n","pruned_client_model.build(input_shape=(None, 40, 1))\n","#print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_client_model)))\n","\n","# Serialize and compress data using pickle\n","def serialize_data(data):\n","    return zlib.compress(pickle.dumps(data))\n","\n","# Client function\n","def start_client(host='127.0.0.1', port=65438):\n","    client_model = create_client_model()\n","    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n","    client_socket.connect((host, port))\n","    print(\"Connected to server!\")\n","\n","    pruning_callback = tfmot.sparsity.keras.UpdatePruningStep()\n","    pruning_callback.set_model(pruned_client_model)\n","    pruning_callback.on_train_begin()\n","\n","    batch_size = 512\n","    epochs =  10\n","    total_batches = int(np.ceil(X_train_scaled.shape[0] / batch_size))\n","    total_comm_overhead = 0\n","\n","    print(\"\\nStarting Training...\")\n","    start_time = time.time()\n","\n","    for epoch in range(epochs):\n","        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n","        epoch_loss = 0\n","        correct_predictions = 0\n","        total_samples = 0\n","\n","        for batch_index, i in enumerate(range(0, X_train_scaled.shape[0], batch_size), start=1):\n","            x_batch = X_train_scaled[i:min(i + batch_size, X_train_scaled.shape[0])]\n","            y_batch = y_train[i:min(i + batch_size, y_train.shape[0])]\n","\n","            pruning_callback.on_train_batch_begin(batch=-1)\n","\n","            # Forward pass to get intermediate activations\n","            intermediate_output = pruned_client_model(x_batch, training=True).numpy()\n","\n","            # Serialize and send data\n","            data = serialize_data((intermediate_output.tolist(), y_batch.tolist()))\n","            data_size = len(data)\n","\n","            client_socket.sendall(f\"{len(data):08}\".encode('utf-8'))\n","            client_socket.sendall(data)\n","\n","            total_comm_overhead += data_size\n","\n","            # Receive server response\n","            response_header = client_socket.recv(8)\n","            response_size = int(response_header.decode('utf-8').strip())\n","            response = client_socket.recv(response_size)\n","            server_loss, batch_correct = map(float, response.decode('utf-8').split(','))\n","            pruning_callback.on_epoch_end(batch=-1)\n","\n","            # Update statistics\n","            epoch_loss += server_loss\n","            correct_predictions += batch_correct\n","            total_samples += x_batch.shape[0]\n","\n","            # Display batch progress\n","            progress = (batch_index / total_batches) * 100\n","            print(f\"\\rBatch {batch_index}/{total_batches} - Progress: {progress:.2f}% - Loss: {server_loss:.4f}\", end=\"\")\n","\n","        training_accuracy = correct_predictions / total_samples\n","        print(f\"\\nEpoch {epoch + 1} completed: Avg Loss = {epoch_loss / total_batches:.4f}, \"\n","              f\"Training Accuracy = {training_accuracy:.4f}\")\n","\n","    total_training_time = time.time() - start_time\n","    print(f\"\\nTraining completed in {total_training_time:.2f} seconds.\")\n","    print(f\"Total Communication Overhead: {total_comm_overhead / 1024:.2f} KB\")\n","\n","    #client_model = tfmot.sparsity.keras.strip_pruning(pruned_client_model)\n","    #client_model.save(\"client_model_pruned.h5\")\n","    model_for_export = tfmot.sparsity.keras.strip_pruning(pruned_client_model)\n","\n","    _, pruned_keras_file = tempfile.mkstemp('.h5')\n","    keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n","    print('Saved pruned Keras model to:', pruned_keras_file)\n","\n","    print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\n","\n","    #converter = tf.lite.TFLiteConverter.from_keras_model(pruned_client_model)\n","    #tflite_model = converter.convert()\n","    #with open(\"client_model_pruned.tflite\", \"wb\") as f:\n","        #f.write(tflite_model)\n","    converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n","    pruned_tflite_model = converter.convert()\n","\n","    _, pruned_tflite_file = tempfile.mkstemp('.tflite')\n","\n","    with open(pruned_tflite_file, 'wb') as f:\n","        f.write(pruned_tflite_model)\n","\n","    print('Saved pruned TFLite model to:', pruned_tflite_file)\n","    print(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(pruned_tflite_file)))\n","\n","        #print(\"Client model saved in .h5 and .tflite formats.\")\n","    # Signal the server to switch to evaluation mode\n","    client_socket.sendall(f\"{-1:08}\".encode('utf-8'))\n","    print(\"Training complete. Signaled server to start evaluation.\")\n","\n","    # Evaluate testing accuracy in batches\n","    print(\"\\nEvaluating on test data...\")\n","    test_batch_size = 512  # Larger batch size for efficient testing\n","    total_test_batches = int(np.ceil(X_test_scaled.shape[0] / test_batch_size))\n","    correct_predictions = 0\n","    total_samples = 0\n","\n","    for i in range(0, X_test_scaled.shape[0], test_batch_size):\n","        x_test_batch = X_test_scaled[i:min(i + test_batch_size, X_test_scaled.shape[0])]\n","        y_test_batch = y_test[i:min(i + test_batch_size, y_test.shape[0])]\n","\n","        test_intermediate = pruned_client_model.predict(x_test_batch, verbose=0)\n","        test_data = serialize_data((test_intermediate.tolist(), y_test_batch.tolist()))\n","\n","        client_socket.sendall(f\"{len(test_data):08}\".encode('utf-8'))\n","        client_socket.sendall(test_data)\n","\n","        # Receive test batch accuracy\n","        test_accuracy_response = client_socket.recv(8)\n","        test_accuracy_size = int(test_accuracy_response.decode('utf-8').strip())\n","        test_accuracy_data = client_socket.recv(test_accuracy_size)\n","        batch_correct = int(test_accuracy_data.decode('utf-8'))\n","\n","        correct_predictions += batch_correct\n","        total_samples += x_test_batch.shape[0]\n","\n","    test_accuracy = correct_predictions / total_samples\n","    print(f\"\\nTesting completed: Testing Accuracy = {test_accuracy:.4f}\")\n","\n","    client_socket.close()\n","    print(\"\\nClient connection closed.\")\n","\n","# Start the client\n","start_client()\n"]},{"cell_type":"markdown","source":["***Pruned Model latest***"],"metadata":{"id":"sfaRDl5mTT_g"}},{"cell_type":"code","source":[],"metadata":{"id":"cLJ2EaF7fbLA"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d_k82M8fUSsi"},"outputs":[],"source":["import socket\n","import pickle\n","import zlib\n","import tensorflow as tf\n","import tf_keras as keras\n","import tensorflow_model_optimization as tfmot\n","import numpy as np\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import MaxPooling1D, Flatten, Dense, BatchNormalization\n","from tensorflow.keras.optimizers import Adam\n","import threading\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","from tensorflow.keras.layers import Dropout\n","# Apply pruning\n","prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n","\n","# Server-side model\n","def create_server_model():\n","    model = keras.Sequential([\n","        keras.layers.MaxPooling1D(pool_size=2,input_shape=(20, 64)),\n","        keras.layers.Conv1D(128, kernel_size=3, activation='relu',padding='same'),\n","        keras.layers.BatchNormalization(),\n","        keras.layers.Conv1D(128, kernel_size=3, activation='relu',padding='same'),\n","        keras.layers.BatchNormalization(),\n","        keras.layers.MaxPooling1D(pool_size=2),\n","        keras.layers.Conv1D(256, kernel_size=3, activation='relu',padding='same'),\n","        keras.layers.BatchNormalization(),\n","        keras.layers.Conv1D(256, kernel_size=3, activation='relu',padding='same'),\n","        keras.layers.BatchNormalization(),\n","        keras.layers.MaxPooling1D(pool_size=2),\n","        keras.layers.Flatten(),\n","        keras.layers.Dense(256, activation='relu'),\n","        keras.layers.Dropout(0.3),\n","        keras.layers.Dense(128, activation='relu'),\n","        keras.layers.Dropout(0.2),\n","        keras.layers.Dense(11, activation='softmax')\n","    ])\n","    return model\n","\n","pruning_params = {\"pruning_schedule\": tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.0, final_sparsity=0.5, begin_step=0, end_step=5530, power=3)}\n","server_model = prune_low_magnitude(create_server_model(), **pruning_params)\n","server_model.build(input_shape=(None, 20, 64))\n","server_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Deserialize and decompress data\n","def deserialize_data(data):\n","    return pickle.loads(zlib.decompress(data))\n","\n","# Server function\n","def start_server(host='127.0.0.1', port=65437):\n","\n","    pruned_server_model=server_model\n","    pruning_callback = tfmot.sparsity.keras.UpdatePruningStep()\n","    log_callback = tfmot.sparsity.keras.PruningSummaries(log_dir=\"logs\")\n","\n","\n","    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n","    server_socket.bind((host, port))\n","    server_socket.listen(1)\n","    print(\"Server waiting for connection...\")\n","    conn, addr = server_socket.accept()\n","    print(f\"Connected to client: {addr}\")\n","    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n","    initial_learning_rate=0.001,\n","    decay_steps=10000,\n","    decay_rate=0.9\n","    )\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n","\n","    total_comm_overhead = 0  # Track communication overhead\n","    pruning_callback.set_model(pruned_server_model)\n","    log_callback.set_model(pruned_server_model)\n","    pruning_callback.on_train_begin()\n","\n","    all_true_labels = []\n","    all_pred_labels = []\n","\n","    training_start_time = time.time()\n","\n","\n","    while True:\n","        try:\n","            # Receive header indicating batch size\n","            header = conn.recv(8)\n","            if not header:\n","                break\n","            data_size = int(header.decode('utf-8').strip())\n","\n","\n","            # If a special flag (e.g., -1) is sent, switch to evaluation\n","            if data_size == -1:\n","              print(\"Switching to evaluation phase...\")\n","              break\n","\n","            # Receive the batch data\n","            data = b\"\"\n","            while len(data) < data_size:\n","                packet = conn.recv(4096)\n","                if not packet:\n","                    break\n","                data += packet\n","            total_comm_overhead += len(data)  # Update comm overhead\n","\n","            # Deserialize the data\n","            intermediate_output, labels = deserialize_data(data)\n","            pruning_callback.on_train_batch_begin(batch=-1)\n","\n","            # Perform forward pass and backpropagation on batch\n","            with tf.GradientTape() as tape:\n","                logits = pruned_server_model(np.array(intermediate_output), training=True)\n","                loss = tf.reduce_mean(\n","                    tf.keras.losses.categorical_crossentropy(np.array(labels), logits)\n","                )\n","            gradients = tape.gradient(loss, pruned_server_model.trainable_variables)\n","            optimizer.apply_gradients(zip(gradients, pruned_server_model.trainable_variables))\n","            pruning_callback.on_epoch_end(batch=-1)\n","\n","            pred_classes = np.argmax(logits.numpy(), axis=1)\n","            true_classes = np.argmax(labels, axis=1)\n","\n","            all_pred_labels.extend(pred_classes)\n","            all_true_labels.extend(true_classes)\n","\n","            correct_predictions = int(np.sum(\n","                np.argmax(logits.numpy(), axis=1) == np.argmax(labels, axis=1)\n","            ))\n","            response = f\"{loss.numpy():.4f},{correct_predictions}\"\n","            conn.sendall(f\"{len(response):08}\".encode('utf-8'))\n","            conn.sendall(response.encode('utf-8'))\n","\n","        except Exception as e:\n","            print(f\"Error: {e}\")\n","            break\n","\n","    training_end_time = time.time()\n","    print(f\"Total Training Time: {training_end_time - training_start_time:.2f} sec\")\n","    print(f\"Total Communication Overhead: {total_comm_overhead / 1024:.2f} KB\")\n","\n","\n","    # At the end of training\n","    precision = precision_score(all_true_labels, all_pred_labels, average='macro')\n","    recall = recall_score(all_true_labels, all_pred_labels, average='macro')\n","    f1 = f1_score(all_true_labels, all_pred_labels, average='macro')\n","    print(f\"\\nTraining Metrics → Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n","\n","    # Strip pruning and save the final model\n","    pruned_server_model = tfmot.sparsity.keras.strip_pruning(pruned_server_model)\n","    server_model.save(\"server_model_pruned.h5\")\n","\n","    converter = tf.lite.TFLiteConverter.from_keras_model(pruned_server_model)\n","    tflite_model = converter.convert()\n","    with open(\"server_model_pruned.tflite\", \"wb\") as f:\n","        f.write(tflite_model)\n","\n","    print(\"Model saved in .h5 and .tflite formats.\")\n","\n","    # Handle test evaluation\n","    print(\"Evaluating test data...\")\n","\n","    all_test_true = []\n","    all_test_pred = []\n","\n","    while True:\n","        header = conn.recv(8)\n","        if not header:\n","            break\n","        test_data_size = int(header.decode('utf-8').strip())\n","        test_data = b\"\"\n","        while len(test_data) < test_data_size:\n","            packet = conn.recv(4096)\n","            if not packet:\n","                break\n","            test_data += packet\n","\n","        test_intermediate, test_labels = deserialize_data(test_data)\n","        #logits = pruned_server_model(np.array(test_intermediate), training=False)\n","\n","        test_intermediate = np.array(test_intermediate)\n","        test_labels = np.array(test_labels)\n","\n","        logits = pruned_server_model(test_intermediate, training=False)\n","        pred_classes = np.argmax(logits.numpy(), axis=1)\n","        true_classes = np.argmax(test_labels, axis=1)\n","\n","        all_test_pred.extend(pred_classes)\n","        all_test_true.extend(true_classes)\n","\n","        correct_predictions = int(np.sum(\n","            np.argmax(logits.numpy(), axis=1) == np.argmax(test_labels, axis=1)\n","        ))\n","        response = f\"{correct_predictions}\"\n","        conn.sendall(f\"{len(response):08}\".encode('utf-8'))\n","        conn.sendall(response.encode('utf-8'))\n","\n","        # Final test metrics\n","        precision = precision_score(all_test_true, all_test_pred, average='macro')\n","        recall = recall_score(all_test_true, all_test_pred, average='macro')\n","        f1 = f1_score(all_test_true, all_test_pred, average='macro')\n","        print(f\"\\nTesting Metrics → Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n","\n","\n","    conn.close()\n","    server_socket.close()\n","    print(\"Server connection closed.\")\n","\n","server_thread = threading.Thread(target=start_server)\n","server_thread.start()\n"]},{"cell_type":"code","source":[],"metadata":{"id":"Ct7nBX_zTR35"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"dc891ee9-af86-4f61-aaea-776dd12008a5","executionInfo":{"status":"ok","timestamp":1746687989449,"user_tz":-330,"elapsed":6180468,"user":{"displayName":"V SANTHOSH KUMAR","userId":"08016301954865386445"}},"collapsed":true,"id":"Ck0hUD0mUXaf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Connected to server!Connected to client: ('127.0.0.1', 40552)\n","\n","\n","Starting Training...\n","\n","Epoch 1/10\n","Batch 553/553 - Progress: 100.00% - Loss: 0.2109\n","Epoch 1 completed: Avg Loss = 0.2581, Training Accuracy = 0.8529\n","\n","Epoch 2/10\n","Batch 553/553 - Progress: 100.00% - Loss: 0.2941\n","Epoch 2 completed: Avg Loss = 0.1847, Training Accuracy = 0.8875\n","\n","Epoch 3/10\n","Batch 553/553 - Progress: 100.00% - Loss: 0.1952\n","Epoch 3 completed: Avg Loss = 0.1942, Training Accuracy = 0.8835\n","\n","Epoch 4/10\n","Batch 553/553 - Progress: 100.00% - Loss: 0.2379\n","Epoch 4 completed: Avg Loss = 0.1945, Training Accuracy = 0.8832\n","\n","Epoch 5/10\n","Batch 553/553 - Progress: 100.00% - Loss: 0.1683\n","Epoch 5 completed: Avg Loss = 0.1744, Training Accuracy = 0.8920\n","\n","Epoch 6/10\n","Batch 553/553 - Progress: 100.00% - Loss: 0.2176\n","Epoch 6 completed: Avg Loss = 0.1907, Training Accuracy = 0.8848\n","\n","Epoch 7/10\n","Batch 553/553 - Progress: 100.00% - Loss: 0.2038\n","Epoch 7 completed: Avg Loss = 0.1863, Training Accuracy = 0.8862\n","\n","Epoch 8/10\n","Batch 553/553 - Progress: 100.00% - Loss: 0.1648\n","Epoch 8 completed: Avg Loss = 0.1607, Training Accuracy = 0.8976\n","\n","Epoch 9/10\n","Batch 553/553 - Progress: 100.00% - Loss: 0.1390\n","Epoch 9 completed: Avg Loss = 0.1434, Training Accuracy = 0.9032\n","\n","Epoch 10/10\n","Batch 552/553 - Progress: 99.82% - Loss: 0.1430"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-41-79275cb49d02>:115: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n","  keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"output_type":"stream","name":"stdout","text":["\rBatch 553/553 - Progress: 100.00% - Loss: 0.1403\n","Epoch 10 completed: Avg Loss = 0.1375, Training Accuracy = 0.9054\n","\n","Training completed in 5899.52 seconds.\n","Total Communication Overhead: 12075253.10 KB\n","Saved pruned Keras model to: /tmp/tmpio582q3m.h5\n","Size of gzipped pruned Keras model: 56753.00 bytes\n","Saved pruned TFLite model to: /tmp/tmpc0lmarnr.tflite\n","Size of gzipped pruned TFlite model: 55664.00 bytes\n","Training complete. Signaled server to start evaluation.\n","\n","Evaluating on test data...\n","Switching to evaluation phase...\n","Total Training Time: 5901.41 sec\n","Total Communication Overhead: 12075253.10 KB\n","\n","Training Metrics → Precision: 0.8878, Recall: 0.8877, F1 Score: 0.8876\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["Model saved in .h5 and .tflite formats.\n","Evaluating test data...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"output_type":"stream","name":"stdout","text":["\n","Testing Metrics → Precision: 0.8479, Recall: 0.8942, F1 Score: 0.8624\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"output_type":"stream","name":"stdout","text":["\n","Testing Metrics → Precision: 0.8514, Recall: 0.8958, F1 Score: 0.8652\n","\n","Testing Metrics → Precision: 0.9424, Recall: 0.8977, F1 Score: 0.8664\n","\n","Testing Metrics → Precision: 0.9415, Recall: 0.8946, F1 Score: 0.8646\n","\n","Testing Metrics → Precision: 0.9408, Recall: 0.8945, F1 Score: 0.8638\n","\n","Testing Metrics → Precision: 0.9410, Recall: 0.8946, F1 Score: 0.8638\n","\n","Testing Metrics → Precision: 0.9411, Recall: 0.8951, F1 Score: 0.8638\n","\n","Testing Metrics → Precision: 0.9408, Recall: 0.8944, F1 Score: 0.8633\n","\n","Testing Metrics → Precision: 0.9401, Recall: 0.8941, F1 Score: 0.8627\n","\n","Testing Metrics → Precision: 0.9400, Recall: 0.8944, F1 Score: 0.8627\n","\n","Testing Metrics → Precision: 0.9400, Recall: 0.8941, F1 Score: 0.8626\n","\n","Testing Metrics → Precision: 0.9390, Recall: 0.8928, F1 Score: 0.8613\n","\n","Testing Metrics → Precision: 0.9388, Recall: 0.8928, F1 Score: 0.8610\n","\n","Testing Metrics → Precision: 0.9394, Recall: 0.8931, F1 Score: 0.8616\n","\n","Testing Metrics → Precision: 0.9396, Recall: 0.8926, F1 Score: 0.8617\n","\n","Testing Metrics → Precision: 0.9397, Recall: 0.8925, F1 Score: 0.8618\n","\n","Testing Metrics → Precision: 0.9398, Recall: 0.8925, F1 Score: 0.8619\n","\n","Testing Metrics → Precision: 0.9402, Recall: 0.8926, F1 Score: 0.8623\n","\n","Testing Metrics → Precision: 0.9409, Recall: 0.8931, F1 Score: 0.8631\n","\n","Testing Metrics → Precision: 0.9408, Recall: 0.8936, F1 Score: 0.8632\n","\n","Testing Metrics → Precision: 0.9407, Recall: 0.8938, F1 Score: 0.8632\n","\n","Testing Metrics → Precision: 0.9405, Recall: 0.8936, F1 Score: 0.8629\n","\n","Testing Metrics → Precision: 0.9404, Recall: 0.8939, F1 Score: 0.8630\n","\n","Testing Metrics → Precision: 0.9406, Recall: 0.8941, F1 Score: 0.8633\n","\n","Testing Metrics → Precision: 0.9404, Recall: 0.8937, F1 Score: 0.8630\n","\n","Testing Metrics → Precision: 0.9403, Recall: 0.8937, F1 Score: 0.8629\n","\n","Testing Metrics → Precision: 0.9404, Recall: 0.8938, F1 Score: 0.8629\n","\n","Testing Metrics → Precision: 0.8951, Recall: 0.8936, F1 Score: 0.8630\n","\n","Testing Metrics → Precision: 0.8950, Recall: 0.8935, F1 Score: 0.8629\n","\n","Testing Metrics → Precision: 0.8953, Recall: 0.8937, F1 Score: 0.8632\n","\n","Testing Metrics → Precision: 0.8955, Recall: 0.8938, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8953, Recall: 0.8940, F1 Score: 0.8633\n","\n","Testing Metrics → Precision: 0.8953, Recall: 0.8940, F1 Score: 0.8632\n","\n","Testing Metrics → Precision: 0.8954, Recall: 0.8939, F1 Score: 0.8633\n","\n","Testing Metrics → Precision: 0.8953, Recall: 0.8938, F1 Score: 0.8632\n","\n","Testing Metrics → Precision: 0.8953, Recall: 0.8936, F1 Score: 0.8632\n","\n","Testing Metrics → Precision: 0.8954, Recall: 0.8938, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8953, Recall: 0.8939, F1 Score: 0.8633\n","\n","Testing Metrics → Precision: 0.8951, Recall: 0.8935, F1 Score: 0.8630\n","\n","Testing Metrics → Precision: 0.8949, Recall: 0.8936, F1 Score: 0.8629\n","\n","Testing Metrics → Precision: 0.8950, Recall: 0.8937, F1 Score: 0.8630\n","\n","Testing Metrics → Precision: 0.8950, Recall: 0.8936, F1 Score: 0.8629\n","\n","Testing Metrics → Precision: 0.8947, Recall: 0.8935, F1 Score: 0.8626\n","\n","Testing Metrics → Precision: 0.8946, Recall: 0.8934, F1 Score: 0.8625\n","\n","Testing Metrics → Precision: 0.8947, Recall: 0.8934, F1 Score: 0.8626\n","\n","Testing Metrics → Precision: 0.8944, Recall: 0.8933, F1 Score: 0.8624\n","\n","Testing Metrics → Precision: 0.8945, Recall: 0.8934, F1 Score: 0.8624\n","\n","Testing Metrics → Precision: 0.8944, Recall: 0.8933, F1 Score: 0.8623\n","\n","Testing Metrics → Precision: 0.8944, Recall: 0.8934, F1 Score: 0.8624\n","\n","Testing Metrics → Precision: 0.8945, Recall: 0.8936, F1 Score: 0.8625\n","\n","Testing Metrics → Precision: 0.8945, Recall: 0.8936, F1 Score: 0.8626\n","\n","Testing Metrics → Precision: 0.8947, Recall: 0.8937, F1 Score: 0.8627\n","\n","Testing Metrics → Precision: 0.8946, Recall: 0.8935, F1 Score: 0.8626\n","\n","Testing Metrics → Precision: 0.8795, Recall: 0.8934, F1 Score: 0.8626\n","\n","Testing Metrics → Precision: 0.8792, Recall: 0.8933, F1 Score: 0.8623\n","\n","Testing Metrics → Precision: 0.8791, Recall: 0.8934, F1 Score: 0.8623\n","\n","Testing Metrics → Precision: 0.8793, Recall: 0.8934, F1 Score: 0.8624\n","\n","Testing Metrics → Precision: 0.8793, Recall: 0.8934, F1 Score: 0.8625\n","\n","Testing Metrics → Precision: 0.8794, Recall: 0.8935, F1 Score: 0.8625\n","\n","Testing Metrics → Precision: 0.8794, Recall: 0.8935, F1 Score: 0.8625\n","\n","Testing Metrics → Precision: 0.8793, Recall: 0.8934, F1 Score: 0.8624\n","\n","Testing Metrics → Precision: 0.8795, Recall: 0.8935, F1 Score: 0.8626\n","\n","Testing Metrics → Precision: 0.8797, Recall: 0.8935, F1 Score: 0.8627\n","\n","Testing Metrics → Precision: 0.8798, Recall: 0.8935, F1 Score: 0.8628\n","\n","Testing Metrics → Precision: 0.8798, Recall: 0.8935, F1 Score: 0.8629\n","\n","Testing Metrics → Precision: 0.8795, Recall: 0.8933, F1 Score: 0.8625\n","\n","Testing Metrics → Precision: 0.8794, Recall: 0.8934, F1 Score: 0.8625\n","\n","Testing Metrics → Precision: 0.8793, Recall: 0.8934, F1 Score: 0.8624\n","\n","Testing Metrics → Precision: 0.8793, Recall: 0.8933, F1 Score: 0.8624\n","\n","Testing Metrics → Precision: 0.8792, Recall: 0.8933, F1 Score: 0.8623\n","\n","Testing Metrics → Precision: 0.8793, Recall: 0.8933, F1 Score: 0.8623\n","\n","Testing Metrics → Precision: 0.8793, Recall: 0.8934, F1 Score: 0.8624\n","\n","Testing Metrics → Precision: 0.8793, Recall: 0.8934, F1 Score: 0.8624\n","\n","Testing Metrics → Precision: 0.8793, Recall: 0.8935, F1 Score: 0.8624\n","\n","Testing Metrics → Precision: 0.8794, Recall: 0.8936, F1 Score: 0.8625\n","\n","Testing Metrics → Precision: 0.8795, Recall: 0.8937, F1 Score: 0.8627\n","\n","Testing Metrics → Precision: 0.8795, Recall: 0.8938, F1 Score: 0.8627\n","\n","Testing Metrics → Precision: 0.8795, Recall: 0.8937, F1 Score: 0.8627\n","\n","Testing Metrics → Precision: 0.8795, Recall: 0.8938, F1 Score: 0.8627\n","\n","Testing Metrics → Precision: 0.8795, Recall: 0.8938, F1 Score: 0.8627\n","\n","Testing Metrics → Precision: 0.8796, Recall: 0.8939, F1 Score: 0.8627\n","\n","Testing Metrics → Precision: 0.8797, Recall: 0.8940, F1 Score: 0.8629\n","\n","Testing Metrics → Precision: 0.8797, Recall: 0.8940, F1 Score: 0.8629\n","\n","Testing Metrics → Precision: 0.8796, Recall: 0.8940, F1 Score: 0.8628\n","\n","Testing Metrics → Precision: 0.8796, Recall: 0.8939, F1 Score: 0.8628\n","\n","Testing Metrics → Precision: 0.8796, Recall: 0.8939, F1 Score: 0.8628\n","\n","Testing Metrics → Precision: 0.8796, Recall: 0.8939, F1 Score: 0.8628\n","\n","Testing Metrics → Precision: 0.8797, Recall: 0.8940, F1 Score: 0.8629\n","\n","Testing Metrics → Precision: 0.8797, Recall: 0.8939, F1 Score: 0.8629\n","\n","Testing Metrics → Precision: 0.8798, Recall: 0.8939, F1 Score: 0.8629\n","\n","Testing Metrics → Precision: 0.8799, Recall: 0.8940, F1 Score: 0.8630\n","\n","Testing Metrics → Precision: 0.8724, Recall: 0.8940, F1 Score: 0.8631\n","\n","Testing Metrics → Precision: 0.8861, Recall: 0.8939, F1 Score: 0.8631\n","\n","Testing Metrics → Precision: 0.8861, Recall: 0.8939, F1 Score: 0.8631\n","\n","Testing Metrics → Precision: 0.8861, Recall: 0.8940, F1 Score: 0.8632\n","\n","Testing Metrics → Precision: 0.8861, Recall: 0.8940, F1 Score: 0.8632\n","\n","Testing Metrics → Precision: 0.8861, Recall: 0.8940, F1 Score: 0.8632\n","\n","Testing Metrics → Precision: 0.8862, Recall: 0.8940, F1 Score: 0.8632\n","\n","Testing Metrics → Precision: 0.8862, Recall: 0.8940, F1 Score: 0.8632\n","\n","Testing Metrics → Precision: 0.8862, Recall: 0.8940, F1 Score: 0.8633\n","\n","Testing Metrics → Precision: 0.8862, Recall: 0.8940, F1 Score: 0.8633\n","\n","Testing Metrics → Precision: 0.8862, Recall: 0.8940, F1 Score: 0.8633\n","\n","Testing Metrics → Precision: 0.8953, Recall: 0.8940, F1 Score: 0.8633\n","\n","Testing Metrics → Precision: 0.8953, Recall: 0.8940, F1 Score: 0.8633\n","\n","Testing Metrics → Precision: 0.8954, Recall: 0.8941, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8953, Recall: 0.8941, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8953, Recall: 0.8941, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8953, Recall: 0.8941, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8953, Recall: 0.8941, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8953, Recall: 0.8940, F1 Score: 0.8633\n","\n","Testing Metrics → Precision: 0.8953, Recall: 0.8941, F1 Score: 0.8633\n","\n","Testing Metrics → Precision: 0.8953, Recall: 0.8940, F1 Score: 0.8633\n","\n","Testing Metrics → Precision: 0.8952, Recall: 0.8941, F1 Score: 0.8633\n","\n","Testing Metrics → Precision: 0.8953, Recall: 0.8941, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8954, Recall: 0.8942, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8955, Recall: 0.8941, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8954, Recall: 0.8941, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8954, Recall: 0.8941, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8956, Recall: 0.8941, F1 Score: 0.8636\n","\n","Testing Metrics → Precision: 0.8955, Recall: 0.8941, F1 Score: 0.8636\n","\n","Testing Metrics → Precision: 0.8954, Recall: 0.8940, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8954, Recall: 0.8940, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8954, Recall: 0.8940, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8954, Recall: 0.8940, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8889, Recall: 0.8940, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8889, Recall: 0.8940, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8889, Recall: 0.8940, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8888, Recall: 0.8939, F1 Score: 0.8633\n","\n","Testing Metrics → Precision: 0.8889, Recall: 0.8940, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8888, Recall: 0.8939, F1 Score: 0.8633\n","\n","Testing Metrics → Precision: 0.8888, Recall: 0.8939, F1 Score: 0.8633\n","\n","Testing Metrics → Precision: 0.8889, Recall: 0.8940, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8889, Recall: 0.8939, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8889, Recall: 0.8939, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8889, Recall: 0.8938, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8890, Recall: 0.8938, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8889, Recall: 0.8938, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8890, Recall: 0.8938, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8891, Recall: 0.8939, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8890, Recall: 0.8938, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8890, Recall: 0.8939, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8890, Recall: 0.8938, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8891, Recall: 0.8939, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8891, Recall: 0.8939, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8891, Recall: 0.8939, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8891, Recall: 0.8939, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8891, Recall: 0.8940, F1 Score: 0.8636\n","\n","Testing Metrics → Precision: 0.8891, Recall: 0.8940, F1 Score: 0.8636\n","\n","Testing Metrics → Precision: 0.8892, Recall: 0.8940, F1 Score: 0.8636\n","\n","Testing Metrics → Precision: 0.8892, Recall: 0.8940, F1 Score: 0.8637\n","\n","Testing Metrics → Precision: 0.8892, Recall: 0.8940, F1 Score: 0.8636\n","\n","Testing Metrics → Precision: 0.8892, Recall: 0.8939, F1 Score: 0.8636\n","\n","Testing Metrics → Precision: 0.8891, Recall: 0.8939, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8891, Recall: 0.8939, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8891, Recall: 0.8939, F1 Score: 0.8636\n","\n","Testing Metrics → Precision: 0.8892, Recall: 0.8939, F1 Score: 0.8636\n","\n","Testing Metrics → Precision: 0.8891, Recall: 0.8939, F1 Score: 0.8636\n","\n","Testing Metrics → Precision: 0.8891, Recall: 0.8939, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8892, Recall: 0.8939, F1 Score: 0.8636\n","\n","Testing Metrics → Precision: 0.8892, Recall: 0.8939, F1 Score: 0.8636\n","\n","Testing Metrics → Precision: 0.8892, Recall: 0.8938, F1 Score: 0.8636\n","\n","Testing Metrics → Precision: 0.8893, Recall: 0.8939, F1 Score: 0.8637\n","\n","Testing Metrics → Precision: 0.8892, Recall: 0.8939, F1 Score: 0.8636\n","\n","Testing Metrics → Precision: 0.8892, Recall: 0.8939, F1 Score: 0.8636\n","\n","Testing Metrics → Precision: 0.8892, Recall: 0.8938, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8891, Recall: 0.8938, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8891, Recall: 0.8938, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8891, Recall: 0.8938, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8892, Recall: 0.8938, F1 Score: 0.8636\n","\n","Testing Metrics → Precision: 0.8892, Recall: 0.8939, F1 Score: 0.8636\n","\n","Testing Metrics → Precision: 0.8891, Recall: 0.8938, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8891, Recall: 0.8938, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8891, Recall: 0.8938, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8891, Recall: 0.8938, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8891, Recall: 0.8938, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8891, Recall: 0.8938, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8891, Recall: 0.8939, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8890, Recall: 0.8938, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8842, Recall: 0.8938, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8842, Recall: 0.8938, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8841, Recall: 0.8937, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8841, Recall: 0.8938, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8841, Recall: 0.8938, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8842, Recall: 0.8937, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8842, Recall: 0.8938, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8842, Recall: 0.8938, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8842, Recall: 0.8938, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8842, Recall: 0.8938, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8842, Recall: 0.8938, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8904, Recall: 0.8939, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8904, Recall: 0.8938, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8904, Recall: 0.8938, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8904, Recall: 0.8938, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8903, Recall: 0.8938, F1 Score: 0.8633\n","\n","Testing Metrics → Precision: 0.8903, Recall: 0.8938, F1 Score: 0.8633\n","\n","Testing Metrics → Precision: 0.8903, Recall: 0.8938, F1 Score: 0.8633\n","\n","Testing Metrics → Precision: 0.8904, Recall: 0.8938, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8904, Recall: 0.8938, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8904, Recall: 0.8938, F1 Score: 0.8633\n","\n","Testing Metrics → Precision: 0.8904, Recall: 0.8939, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8904, Recall: 0.8939, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8904, Recall: 0.8939, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8903, Recall: 0.8938, F1 Score: 0.8633\n","\n","Testing Metrics → Precision: 0.8904, Recall: 0.8938, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8904, Recall: 0.8939, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8905, Recall: 0.8939, F1 Score: 0.8634\n","\n","Testing Metrics → Precision: 0.8905, Recall: 0.8939, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8905, Recall: 0.8939, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8905, Recall: 0.8939, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8905, Recall: 0.8939, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8905, Recall: 0.8940, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8905, Recall: 0.8939, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8905, Recall: 0.8940, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8905, Recall: 0.8940, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8905, Recall: 0.8939, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8906, Recall: 0.8939, F1 Score: 0.8635\n","\n","Testing Metrics → Precision: 0.8906, Recall: 0.8939, F1 Score: 0.8636\n","\n","Testing Metrics → Precision: 0.8906, Recall: 0.8939, F1 Score: 0.8636\n","\n","Testing Metrics → Precision: 0.8907, Recall: 0.8939, F1 Score: 0.8636\n","\n","Testing Metrics → Precision: 0.8907, Recall: 0.8939, F1 Score: 0.8636\n","\n","Testing Metrics → Precision: 0.8907, Recall: 0.8939, F1 Score: 0.8636\n","\n","Testing Metrics → Precision: 0.8907, Recall: 0.8940, F1 Score: 0.8637\n","\n","Testing Metrics → Precision: 0.8907, Recall: 0.8940, F1 Score: 0.8637\n","\n","Testing Metrics → Precision: 0.8907, Recall: 0.8940, F1 Score: 0.8637\n","\n","Testing Metrics → Precision: 0.8907, Recall: 0.8940, F1 Score: 0.8637\n","\n","Testing Metrics → Precision: 0.8907, Recall: 0.8940, F1 Score: 0.8637\n","\n","Testing Metrics → Precision: 0.8907, Recall: 0.8940, F1 Score: 0.8637\n","\n","Testing Metrics → Precision: 0.8907, Recall: 0.8940, F1 Score: 0.8636\n","\n","Testing Metrics → Precision: 0.8907, Recall: 0.8940, F1 Score: 0.8637\n","\n","Testing Metrics → Precision: 0.8907, Recall: 0.8940, F1 Score: 0.8637\n","\n","Testing Metrics → Precision: 0.8907, Recall: 0.8940, F1 Score: 0.8637\n","\n","Testing Metrics → Precision: 0.8907, Recall: 0.8939, F1 Score: 0.8636\n","\n","Testing Metrics → Precision: 0.8907, Recall: 0.8940, F1 Score: 0.8636\n","\n","Testing Metrics → Precision: 0.8906, Recall: 0.8939, F1 Score: 0.8636\n","\n","Testing Metrics → Precision: 0.8906, Recall: 0.8939, F1 Score: 0.8636\n","\n","Testing Metrics → Precision: 0.8906, Recall: 0.8939, F1 Score: 0.8636\n","\n","Testing completed: Testing Accuracy = 0.8938\n","\n","Client connection closed.\n"]}],"source":["import socket\n","import pickle\n","import zlib\n","import time\n","import tensorflow as tf\n","import numpy as np\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical\n","\n","# Client-side model\n","def create_client_model():\n","    model =keras.Sequential([\n","        keras.layers.InputLayer(input_shape=(40, 1)),\n","        keras.layers.Conv1D(16, kernel_size=3, activation='relu',padding='same'),\n","        keras.layers.BatchNormalization(),\n","        keras.layers.Conv1D(32, kernel_size=3, activation='relu',padding='same'),\n","        keras.layers.BatchNormalization(),\n","        #MaxPooling1D(pool_size=2),\n","        keras.layers.Conv1D(32, kernel_size=3, activation='relu',padding='same'),\n","        keras.layers.BatchNormalization(),\n","        #MaxPooling1D(pool_size=2),\n","        keras.layers.Conv1D(64, kernel_size=3, activation='relu',padding='same'),\n","        keras.layers.BatchNormalization(),\n","        keras.layers.Conv1D(64, kernel_size=3, activation='relu',padding='same'),\n","        keras.layers.BatchNormalization(),\n","        keras.layers.MaxPooling1D(pool_size=2),\n","\n","    ])\n","    return model\n","\n","    # Apply pruning\n","pruning_params = {'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.0, final_sparsity=0.50, begin_step=0, end_step=5530,power=3)}\n","pruned_client_model = prune_low_magnitude(create_client_model(), **pruning_params)\n","pruned_client_model.build(input_shape=(None, 40, 1))\n","#print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_client_model)))\n","\n","# Serialize and compress data using pickle\n","def serialize_data(data):\n","    return zlib.compress(pickle.dumps(data))\n","\n","# Client function\n","def start_client(host='127.0.0.1', port=65437):\n","    client_model = create_client_model()\n","    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n","    client_socket.connect((host, port))\n","    print(\"Connected to server!\")\n","\n","    pruning_callback = tfmot.sparsity.keras.UpdatePruningStep()\n","    pruning_callback.set_model(pruned_client_model)\n","    pruning_callback.on_train_begin()\n","\n","    batch_size = 512\n","    epochs =  10\n","    total_batches = int(np.ceil(X_train_scaled.shape[0] / batch_size))\n","    total_comm_overhead = 0\n","\n","    print(\"\\nStarting Training...\")\n","    start_time = time.time()\n","\n","    for epoch in range(epochs):\n","        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n","        epoch_loss = 0\n","        correct_predictions = 0\n","        total_samples = 0\n","\n","        for batch_index, i in enumerate(range(0, X_train_scaled.shape[0], batch_size), start=1):\n","            x_batch = X_train_scaled[i:min(i + batch_size, X_train_scaled.shape[0])]\n","            y_batch = y_train[i:min(i + batch_size, y_train.shape[0])]\n","\n","            pruning_callback.on_train_batch_begin(batch=-1)\n","\n","            # Forward pass to get intermediate activations\n","            intermediate_output = pruned_client_model(x_batch, training=True).numpy()\n","\n","            # Serialize and send data\n","            data = serialize_data((intermediate_output.tolist(), y_batch.tolist()))\n","            data_size = len(data)\n","\n","            client_socket.sendall(f\"{len(data):08}\".encode('utf-8'))\n","            client_socket.sendall(data)\n","\n","            total_comm_overhead += data_size\n","\n","            # Receive server response\n","            response_header = client_socket.recv(8)\n","            response_size = int(response_header.decode('utf-8').strip())\n","            response = client_socket.recv(response_size)\n","            server_loss, batch_correct = map(float, response.decode('utf-8').split(','))\n","            pruning_callback.on_epoch_end(batch=-1)\n","\n","            # Update statistics\n","            epoch_loss += server_loss\n","            correct_predictions += batch_correct\n","            total_samples += x_batch.shape[0]\n","\n","            # Display batch progress\n","            progress = (batch_index / total_batches) * 100\n","            print(f\"\\rBatch {batch_index}/{total_batches} - Progress: {progress:.2f}% - Loss: {server_loss:.4f}\", end=\"\")\n","\n","        training_accuracy = correct_predictions / total_samples\n","        print(f\"\\nEpoch {epoch + 1} completed: Avg Loss = {epoch_loss / total_batches:.4f}, \"\n","              f\"Training Accuracy = {training_accuracy:.4f}\")\n","\n","    total_training_time = time.time() - start_time\n","    print(f\"\\nTraining completed in {total_training_time:.2f} seconds.\")\n","    print(f\"Total Communication Overhead: {total_comm_overhead / 1024:.2f} KB\")\n","\n","    #client_model = tfmot.sparsity.keras.strip_pruning(pruned_client_model)\n","    #client_model.save(\"client_model_pruned.h5\")\n","    model_for_export = tfmot.sparsity.keras.strip_pruning(pruned_client_model)\n","\n","    _, pruned_keras_file = tempfile.mkstemp('.h5')\n","    keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n","    print('Saved pruned Keras model to:', pruned_keras_file)\n","\n","    print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\n","\n","    #converter = tf.lite.TFLiteConverter.from_keras_model(pruned_client_model)\n","    #tflite_model = converter.convert()\n","    #with open(\"client_model_pruned.tflite\", \"wb\") as f:\n","        #f.write(tflite_model)\n","    converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n","    pruned_tflite_model = converter.convert()\n","\n","    _, pruned_tflite_file = tempfile.mkstemp('.tflite')\n","\n","    with open(pruned_tflite_file, 'wb') as f:\n","        f.write(pruned_tflite_model)\n","\n","    print('Saved pruned TFLite model to:', pruned_tflite_file)\n","    print(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(pruned_tflite_file)))\n","\n","        #print(\"Client model saved in .h5 and .tflite formats.\")\n","    # Signal the server to switch to evaluation mode\n","    client_socket.sendall(f\"{-1:08}\".encode('utf-8'))\n","    print(\"Training complete. Signaled server to start evaluation.\")\n","\n","    # Evaluate testing accuracy in batches\n","    print(\"\\nEvaluating on test data...\")\n","    test_batch_size = 512  # Larger batch size for efficient testing\n","    total_test_batches = int(np.ceil(X_test_scaled.shape[0] / test_batch_size))\n","    correct_predictions = 0\n","    total_samples = 0\n","\n","    for i in range(0, X_test_scaled.shape[0], test_batch_size):\n","        x_test_batch = X_test_scaled[i:min(i + test_batch_size, X_test_scaled.shape[0])]\n","        y_test_batch = y_test[i:min(i + test_batch_size, y_test.shape[0])]\n","\n","        test_intermediate = pruned_client_model.predict(x_test_batch, verbose=0)\n","        test_data = serialize_data((test_intermediate.tolist(), y_test_batch.tolist()))\n","\n","        client_socket.sendall(f\"{len(test_data):08}\".encode('utf-8'))\n","        client_socket.sendall(test_data)\n","\n","        # Receive test batch accuracy\n","        test_accuracy_response = client_socket.recv(8)\n","        test_accuracy_size = int(test_accuracy_response.decode('utf-8').strip())\n","        test_accuracy_data = client_socket.recv(test_accuracy_size)\n","        batch_correct = int(test_accuracy_data.decode('utf-8'))\n","\n","        correct_predictions += batch_correct\n","        total_samples += x_test_batch.shape[0]\n","\n","    test_accuracy = correct_predictions / total_samples\n","    print(f\"\\nTesting completed: Testing Accuracy = {test_accuracy:.4f}\")\n","\n","    client_socket.close()\n","    print(\"\\nClient connection closed.\")\n","\n","# Start the client\n","start_client()\n"]},{"cell_type":"code","source":["model_for_export = tfmot.sparsity.keras.strip_pruning(pruned_client_model)\n","\n","_, pruned_keras_file = tempfile.mkstemp('.h5')\n","keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n","print('Saved pruned Keras model to:', pruned_keras_file)\n","\n","    #converter = tf.lite.TFLiteConverter.from_keras_model(pruned_client_model)\n","    #tflite_model = converter.convert()\n","    #with open(\"client_model_pruned.tflite\", \"wb\") as f:\n","        #f.write(tflite_model)\n","converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n","pruned_tflite_model = converter.convert()\n","_, pruned_tflite_file = tempfile.mkstemp('.tflite')\n","with open(pruned_tflite_file, 'wb') as f:\n","      f.write(pruned_tflite_model)\n","print('Saved pruned TFLite model to:', pruned_tflite_file)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AiXY4fdYTR7C","executionInfo":{"status":"ok","timestamp":1746688081618,"user_tz":-330,"elapsed":2187,"user":{"displayName":"V SANTHOSH KUMAR","userId":"08016301954865386445"}},"outputId":"45c826d3-60a4-459b-c85e-6baf4e0d30df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-42-034164553681>:4: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n","  keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"output_type":"stream","name":"stdout","text":["Saved pruned Keras model to: /tmp/tmp_owri41w.h5\n","Saved pruned TFLite model to: /tmp/tmp4n6atqef.tflite\n"]}]},{"cell_type":"code","source":["# Create and save the model\n","client_model = create_client_model()\n","client_model.build(input_shape=(None, 40, 1))  # Ensure model is built\n","model_path = \"client_model.keras\"  # Save using the new format\n","client_model.save(model_path)"],"metadata":{"id":"PzQ88nX8TR9_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(model_path)))\n","print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\n","print(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(pruned_tflite_file)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LchIoIzoTSBB","executionInfo":{"status":"ok","timestamp":1746688132069,"user_tz":-330,"elapsed":63,"user":{"displayName":"V SANTHOSH KUMAR","userId":"08016301954865386445"}},"outputId":"af46b5e1-28db-4b76-bcca-e5cbb9582d5b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Size of gzipped baseline Keras model: 87346.00 bytes\n","Size of gzipped pruned Keras model: 56753.00 bytes\n","Size of gzipped pruned TFlite model: 55664.00 bytes\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# Load the stripped pruned model\n","model = tf.keras.models.load_model(\"/tmp/tmp_owri41w.h5\")\n","\n","# Convert to TFLite with weight quantization\n","converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Enables weight quantization\n","tflite_model = converter.convert()\n","\n","_, quantized_tflite_file = tempfile.mkstemp('.tflite')\n","\n","with open(quantized_tflite_file, 'wb') as f:\n","      f.write(tflite_model)\n","\n","print('Saved Quantized TFLite model to:', quantized_tflite_file)\n","\n","# Save the TFLite model\n","#with open(\"client_model_quantized.tflite\", \"wb\") as f:\n","    #f.write(tflite_model)\n","\n","print(\"Quantized TFLite model saved successfully!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lb86KXZKTSEI","executionInfo":{"status":"ok","timestamp":1746688272309,"user_tz":-330,"elapsed":1515,"user":{"displayName":"V SANTHOSH KUMAR","userId":"08016301954865386445"}},"outputId":"128305b6-5232-427f-eb04-6cfdeafe054b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"]},{"output_type":"stream","name":"stdout","text":["Saved artifact at '/tmp/tmpi68htuqe'. The following endpoints are available:\n","\n","* Endpoint 'serve'\n","  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 40, 1), dtype=tf.float32, name='input_1')\n","Output Type:\n","  TensorSpec(shape=(None, 20, 64), dtype=tf.float32, name=None)\n","Captures:\n","  138647786869520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138647786877200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138647786869328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138647786876816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138647786871440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138647786874704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138647786879696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138647786876624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138647786876048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138647786877008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138647786877776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138647786878928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138647786874512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138647786872208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138647786868944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138647786868560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138647786880272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138647786879120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138647786873936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138647571610128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138647571609168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138645239280080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138652627810832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138647786873168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138645239277008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138645239283152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138645239278160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138645239283536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138645239282384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138645239269520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","Saved Quantized TFLite model to: /tmp/tmpdsv13p66.tflite\n","Quantized TFLite model saved successfully!\n"]}]},{"cell_type":"code","source":["print(\"Size of gzipped quantized TFlite model: %.2f bytes\" % (get_gzipped_model_size(quantized_tflite_file)))"],"metadata":{"id":"Cg4NUWjcJ47b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746688308277,"user_tz":-330,"elapsed":132,"user":{"displayName":"V SANTHOSH KUMAR","userId":"08016301954865386445"}},"outputId":"ec85fbe8-78a8-48ff-989f-3133f2a9f486"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Size of gzipped quantized TFlite model: 21096.00 bytes\n"]}]},{"cell_type":"code","source":["# Load the stripped pruned model\n","model = tf.keras.models.load_model(\"/tmp/tmp_owri41w.h5\")\n","\n","# Convert to TFLite with weight quantization\n","converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]  # float16 quantization\n","converter.target_spec.supported_types = [tf.float16]\n","tflite_model = converter.convert()\n","\n","_, quantized_tflite_file = tempfile.mkstemp('.tflite')\n","\n","with open(quantized_tflite_file, 'wb') as f:\n","      f.write(tflite_model)\n","\n","print('Saved Quantized TFLite model to:', quantized_tflite_file)\n","\n","# Save the TFLite model\n","#with open(\"client_model_quantized.tflite\", \"wb\") as f:\n","    #f.write(tflite_model)\n","\n","print(\"Quantized TFLite model saved successfully!\")"],"metadata":{"id":"x1sUs4nVU9oQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746688415561,"user_tz":-330,"elapsed":1840,"user":{"displayName":"V SANTHOSH KUMAR","userId":"08016301954865386445"}},"outputId":"7215d6fc-039f-4428-f725-8a332d55568f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"]},{"output_type":"stream","name":"stdout","text":["Saved artifact at '/tmp/tmp9iibndfv'. The following endpoints are available:\n","\n","* Endpoint 'serve'\n","  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 40, 1), dtype=tf.float32, name='input_1')\n","Output Type:\n","  TensorSpec(shape=(None, 20, 64), dtype=tf.float32, name=None)\n","Captures:\n","  138641546064144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138641546062608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138641546063760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138641546062032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138641546052624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138641546062800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138641546061648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138641546060304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138641546060880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138641546059728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138641546061072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138641546060496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138641546059344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138641546058000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138641546058576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138641546057424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138641546058768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138641546058192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138641546056272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138641546055504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138641546058384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138641546054928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138641546057040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138641546055696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138641546055888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138641546052816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138641546056080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138641546064720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138641546053776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  138641546053008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","Saved Quantized TFLite model to: /tmp/tmpi66iwtvw.tflite\n","Quantized TFLite model saved successfully!\n"]}]},{"cell_type":"code","source":["print(\"Size of gzipped quantized TFlite model: %.2f bytes\" % (get_gzipped_model_size(quantized_tflite_file)))"],"metadata":{"id":"OfPSs1e7U9rs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746688429525,"user_tz":-330,"elapsed":80,"user":{"displayName":"V SANTHOSH KUMAR","userId":"08016301954865386445"}},"outputId":"64358eb1-d240-4aca-8467-0aa69a73a2cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Size of gzipped quantized TFlite model: 32093.00 bytes\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"t7LLyviMJ4_t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"dbReDqARJ5EP"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JaMyVRN7l_in"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8pevU2Y6l_ld"},"outputs":[],"source":[]}],"metadata":{"colab":{"gpuType":"L4","provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyP0poxUnp3hhh9uHtGRoI7D"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}